{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU9C88kzStvH",
        "outputId": "3405823c-b211-4d59-b0ae-cf2ed8e3b2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/571.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m563.2/571.1 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.1/571.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain --quiet\n",
        "!pip install langchain-openai --quiet\n",
        "!pip install langchain-community --quiet\n",
        "!pip install streamlit --quiet\n",
        "!pip install PyMuPDF --quiet\n",
        "!pip install chromadb --quiet\n",
        "!pip install pyngrok --quiet\n",
        "pip install pypdf --quiet\n",
        "!pip install langchain-community --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install langchain_chroma --quiet\n",
        "!pip install langgraph --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcFPXsjScOuX",
        "outputId": "61646499-25a8-428e-c45d-9e05077057f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_API_KEY')\n",
        "os.environ['NGROK_AUTH_TOKEN'] = userdata.get('NGROK_AUTH_TOKEN')\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "Xq_OrdrNT-Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import tempfile\n",
        "import streamlit as st\n",
        "from operator import itemgetter\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------------------\n",
        "# Page configuration\n",
        "# ----------------------------------\n",
        "st.set_page_config(page_title=\"File QA Chatbot\", page_icon=\"🤖\")\n",
        "st.title(\"Let's Chat With Your Files 🤖\")\n",
        "\n",
        "# ----------------------------------\n",
        "# API Keys setup (via st.secrets or environment)\n",
        "# ----------------------------------\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPEN_API_KEY')\n",
        "\n",
        "\n",
        "# ----------------------------------\n",
        "# Original RAG Pipeline Code (preserved)\n",
        "# ----------------------------------\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langgraph.graph import END, StateGraph\n",
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Initialize embedding model\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "\n",
        "# Sidebar: Multiple PDF upload\n",
        "st.sidebar.header(\"Upload PDFs\")\n",
        "uploaded_files = st.sidebar.file_uploader(\n",
        "    \"Upload one or more PDF files\", type=\"pdf\", accept_multiple_files=True\n",
        ")\n",
        "\n",
        "pdf_paths = []\n",
        "if uploaded_files:\n",
        "    st.sidebar.write(f\"Uploading {len(uploaded_files)} file(s)...\")\n",
        "    for file in uploaded_files:\n",
        "        tfile = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\")\n",
        "        tfile.write(file.getvalue())\n",
        "        pdf_paths.append(tfile.name)\n",
        "\n",
        "if not pdf_paths:\n",
        "    st.info(\"Please upload PDF files in the sidebar to begin.\")\n",
        "    st.stop()\n",
        "\n",
        "# Load PDFs and split into documents\n",
        "all_docs = []\n",
        "for path in pdf_paths:\n",
        "    loader = PyPDFLoader(path)\n",
        "    pdf_docs = loader.load()  # Each page is a separate Document\n",
        "    all_docs.extend(pdf_docs)\n",
        "#st.write(f\"Loaded **{len(all_docs)}** total pages from PDFs.\")\n",
        "\n",
        "# Chunk the documents (~1000 characters with 100 overlap)\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "chunked_docs = text_splitter.split_documents(all_docs)\n",
        "#st.write(f\"Total chunks after splitting: **{len(chunked_docs)}**\")\n",
        "\n",
        "# Create vector DB of docs and embeddings\n",
        "persist_directory = \"./papers_db\"\n",
        "chroma_db = Chroma.from_documents(\n",
        "    documents=chunked_docs,\n",
        "    collection_name='papers',\n",
        "    embedding=openai_embed_model,\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "similarity_threshold_retriever = chroma_db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 3, \"score_threshold\": 0.3}\n",
        ")\n",
        "\n",
        "# Define grading LLM and prompt for document relevance\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "llm_grader = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "structured_llm_grader = llm_grader.with_structured_output(GradeDocuments)\n",
        "\n",
        "SYS_PROMPT = (\n",
        "    \"You are an expert grader assessing relevance of a retrieved document to a user question.\\n\"\n",
        "    \"Follow these instructions for grading:\\n\"\n",
        "    \"  - If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\\n\"\n",
        "    \"  - Your grade should be either 'yes' or 'no' to indicate whether the document is relevant to the question or not.\"\n",
        ")\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        (\"human\", \"\"\"Retrieved document:\n",
        "{document}\n",
        "User question:\n",
        "{question}\n",
        "\"\"\"),\n",
        "    ]\n",
        ")\n",
        "doc_grader = grade_prompt | structured_llm_grader\n",
        "\n",
        "# Create QA generation chain components\n",
        "qa_prompt_template = \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If no context is present or if you don't know the answer, just say that you don't know the answer.\n",
        "Do not make up the answer unless it is there in the provided context.\n",
        "Give the answer to the point with regard to the question no more than three lines.\n",
        "Question:\n",
        "{question}\n",
        "Context:\n",
        "{context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_template(qa_prompt_template)\n",
        "chatgpt = ChatOpenAI(model_name='gpt-4o', temperature=0)\n",
        "\n",
        "def format_docs(docs: List[Document]) -> str:\n",
        "    if hasattr(docs[0], 'content'):\n",
        "        return \"\\n\\n\".join(doc.content for doc in docs)\n",
        "    elif hasattr(docs[0], 'page_content'):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "    return \"\"\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (itemgetter('context') | RunnableLambda(format_docs)),\n",
        "        \"question\": itemgetter('question')\n",
        "    }\n",
        "    | prompt_template\n",
        "    | chatgpt\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Define the agentic RAG pipeline graph using StateGraph\n",
        "class GraphState(TypedDict):\n",
        "    question: str\n",
        "    generation: str\n",
        "    web_search_needed: str\n",
        "    documents: List[Document]\n",
        "\n",
        "def retrieve(state):\n",
        "    st.write(\"**[Retrieval from Vector DB]**\")\n",
        "    question = state[\"question\"]\n",
        "    documents = similarity_threshold_retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "def grade_documents(state):\n",
        "    st.write(\"**[Checking Document Relevance]**\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    filtered_docs = []\n",
        "    web_search_needed = \"No\"\n",
        "    if documents:\n",
        "        for d in documents:\n",
        "            score = doc_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
        "            if score.binary_score.lower() == \"yes\":\n",
        "                st.write(\"Document graded: **Relevant**\")\n",
        "                filtered_docs.append(d)\n",
        "            else:\n",
        "                st.write(\"Document graded: **Not Relevant**\")\n",
        "                web_search_needed = \"Yes\"\n",
        "        # End for\n",
        "    else:\n",
        "        st.write(\"No documents retrieved.\")\n",
        "        web_search_needed = \"Yes\"\n",
        "    return {\"documents\": filtered_docs, \"question\": question, \"web_search_needed\": web_search_needed}\n",
        "\n",
        "def rewrite_query(state):\n",
        "    st.write(\"**[Rewriting Query]**\")\n",
        "    # Here you could add a rewriting chain. For now, we simply pass through.\n",
        "    question = state[\"question\"]\n",
        "    return {\"documents\": state[\"documents\"], \"question\": question}\n",
        "\n",
        "def web_search(state):\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI()\n",
        "    st.write(\"**[Performing Web Search]**\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-search-preview\",\n",
        "        web_search_options={\"search_context_size\": \"low\"},\n",
        "        messages=[{\"role\": \"user\", \"content\": question}]\n",
        "    )\n",
        "    web_results = completion.choices[0].message\n",
        "    # Append the web result to documents (assumes it has a page_content attribute)\n",
        "    documents.append(web_results)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "def generate_answer(state):\n",
        "    st.write(\"**[Generating Answer]**\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = qa_rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    st.write(\"**[Deciding Next Step]**\")\n",
        "    if state[\"web_search_needed\"] == \"Yes\":\n",
        "        st.write(\"Rewriting query due to non-relevant documents.\")\n",
        "        return \"rewrite_query\"\n",
        "    else:\n",
        "        st.write(\"Proceeding to generate answer.\")\n",
        "        return \"generate_answer\"\n",
        "\n",
        "agentic_rag = StateGraph(GraphState)\n",
        "agentic_rag.add_node(\"retrieve\", retrieve)\n",
        "agentic_rag.add_node(\"grade_documents\", grade_documents)\n",
        "agentic_rag.add_node(\"rewrite_query\", rewrite_query)\n",
        "agentic_rag.add_node(\"web_search\", web_search)\n",
        "agentic_rag.add_node(\"generate_answer\", generate_answer)\n",
        "agentic_rag.set_entry_point(\"retrieve\")\n",
        "agentic_rag.add_edge(\"retrieve\", \"grade_documents\")\n",
        "agentic_rag.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\"rewrite_query\": \"rewrite_query\", \"generate_answer\": \"generate_answer\"},\n",
        ")\n",
        "agentic_rag.add_edge(\"rewrite_query\", \"web_search\")\n",
        "agentic_rag.add_edge(\"web_search\", \"generate_answer\")\n",
        "agentic_rag.add_edge(\"generate_answer\", END)\n",
        "agentic_rag = agentic_rag.compile()\n",
        "\n",
        "# ----------------------------------\n",
        "# Chatbot UI using StreamlitChatMessageHistory\n",
        "# ----------------------------------\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "\n",
        "# Initialize conversation history using StreamlitChatMessageHistory\n",
        "streamlit_msg_history = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
        "\n",
        "# Shows the first message when the app starts\n",
        "if len(streamlit_msg_history.messages) == 0:\n",
        "    streamlit_msg_history.add_ai_message(\"What would you like to ask to your documents?\")\n",
        "\n",
        "# Render existing conversation messages\n",
        "for msg in streamlit_msg_history.messages:\n",
        "    st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "# Helper function to compile chat history into a string (if needed)\n",
        "def compile_chat_history(messages):\n",
        "    history = \"\"\n",
        "    for msg in messages:\n",
        "        role = msg.type.capitalize()\n",
        "        history += f\"{role}: {msg.content}\\n\"\n",
        "    return history\n",
        "\n",
        "# Chat input and processing\n",
        "if user_prompt := st.chat_input(\"Enter your question:\"):\n",
        "    # Display user message and add it to message history\n",
        "    st.chat_message(\"user\").write(user_prompt)\n",
        "    streamlit_msg_history.add_user_message(user_prompt)\n",
        "\n",
        "    # Optionally compile chat history (if you want to include it in the prompt)\n",
        "    chat_history_text = compile_chat_history(streamlit_msg_history.messages)\n",
        "\n",
        "    # Invoke the agentic RAG pipeline with the current question\n",
        "    state = {\"question\": user_prompt}\n",
        "    result = agentic_rag.invoke(state)\n",
        "    answer = result.get(\"generation\", \"Sorry, I could not generate an answer.\")\n",
        "\n",
        "    # Display bot response and update conversation history\n",
        "    st.chat_message(\"assistant\").write(answer)\n",
        "    streamlit_msg_history.add_ai_message(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q4W18YCSzpb",
        "outputId": "43fc7a9a-ddfd-469b-868f-6a61e7af4e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "shBMb0Q3Tpt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import yaml\n",
        "import os\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken\n",
        "# Get your authtoken from `ngrok_credentials.yml` file\n",
        "# with open('ngrok_credentials.yml', 'r') as file:\n",
        "#     NGROK_AUTH_TOKEN = yaml.safe_load(file)\n",
        "ngrok.set_auth_token(os.environ['NGROK_AUTH_TOKEN'] )\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GWrRpJKTsGq",
        "outputId": "a5e6771f-071e-4296-f22b-f4b1dad300b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://1679-35-230-16-184.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}