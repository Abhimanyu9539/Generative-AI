{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e876a59314b41d9ba2cf41ea5410156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80980f880db143ae96ba9c8bba89915b",
              "IPY_MODEL_97d22d99e32c4d51b7c5767f2e636e56",
              "IPY_MODEL_d4e341ae57e9415e970bc37684e6b7a1"
            ],
            "layout": "IPY_MODEL_3335a8a8fafc4cbbb326ac4505a4e2d6"
          }
        },
        "80980f880db143ae96ba9c8bba89915b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f99dac7635744b7a9b904979ec9e3a03",
            "placeholder": "​",
            "style": "IPY_MODEL_d8d45be0daed4088a6b6ea4305fa241e",
            "value": "config.json: 100%"
          }
        },
        "97d22d99e32c4d51b7c5767f2e636e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dbc8e2c067b4c9b8eab4bcf50c72c3c",
            "max": 801,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d965ee0ef9f407a9bb09c4bd3dabb06",
            "value": 801
          }
        },
        "d4e341ae57e9415e970bc37684e6b7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d054926ea744273ab80e3f581ee4111",
            "placeholder": "​",
            "style": "IPY_MODEL_eab3687daa4c4ae6951fed58320c5fec",
            "value": " 801/801 [00:00&lt;00:00, 66.1kB/s]"
          }
        },
        "3335a8a8fafc4cbbb326ac4505a4e2d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f99dac7635744b7a9b904979ec9e3a03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d45be0daed4088a6b6ea4305fa241e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dbc8e2c067b4c9b8eab4bcf50c72c3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d965ee0ef9f407a9bb09c4bd3dabb06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d054926ea744273ab80e3f581ee4111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eab3687daa4c4ae6951fed58320c5fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a587ee0b7b845e29dc852a96b5348e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a5450b69950406385388f139714eb24",
              "IPY_MODEL_95088ffb6428449e8ced3b67c474ab05",
              "IPY_MODEL_49fe299baac047bf8cea0882d4a3177c"
            ],
            "layout": "IPY_MODEL_4c1166e1d7184c6081c8eab2bf958628"
          }
        },
        "2a5450b69950406385388f139714eb24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11bccc39c44b48cab0b67735dc5a6541",
            "placeholder": "​",
            "style": "IPY_MODEL_c5ac57f8c2164bf2b43461e738f12b21",
            "value": "model.safetensors: 100%"
          }
        },
        "95088ffb6428449e8ced3b67c474ab05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87cc4dae73784607b563fb785fadea6a",
            "max": 2239618772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6a14c52c82040d5aa95c35e0f10de33",
            "value": 2239618772
          }
        },
        "49fe299baac047bf8cea0882d4a3177c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a95cd26c40e4c27884da84bc98f2c01",
            "placeholder": "​",
            "style": "IPY_MODEL_784af4222b884d0aaf5fc0ba04a96498",
            "value": " 2.24G/2.24G [00:22&lt;00:00, 136MB/s]"
          }
        },
        "4c1166e1d7184c6081c8eab2bf958628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11bccc39c44b48cab0b67735dc5a6541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5ac57f8c2164bf2b43461e738f12b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87cc4dae73784607b563fb785fadea6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6a14c52c82040d5aa95c35e0f10de33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a95cd26c40e4c27884da84bc98f2c01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "784af4222b884d0aaf5fc0ba04a96498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c981b47c36f14bcc9907622b2ef53c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a38179a527484bc9b6408833ff9b8ecc",
              "IPY_MODEL_4528cf6fc66342269e539180f8876423",
              "IPY_MODEL_d301cddc5d42454fa4e1dbb3b7dcd56b"
            ],
            "layout": "IPY_MODEL_1e0daf05868f4b7baef91170df3c162d"
          }
        },
        "a38179a527484bc9b6408833ff9b8ecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f0171ea4bb844fcbe2e1ee28aa0e2a0",
            "placeholder": "​",
            "style": "IPY_MODEL_46be3e17a850430d8e5a971bde90f4ee",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4528cf6fc66342269e539180f8876423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3210eeea849c41adb1ebfc3424c8e2e2",
            "max": 443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65cb0f0b6b9a4a37ac23d3badac20932",
            "value": 443
          }
        },
        "d301cddc5d42454fa4e1dbb3b7dcd56b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d92327c875349d8b51688a2b7ae1f4f",
            "placeholder": "​",
            "style": "IPY_MODEL_baceb40e419141bd839e42fa62f9b71b",
            "value": " 443/443 [00:00&lt;00:00, 41.9kB/s]"
          }
        },
        "1e0daf05868f4b7baef91170df3c162d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0171ea4bb844fcbe2e1ee28aa0e2a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46be3e17a850430d8e5a971bde90f4ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3210eeea849c41adb1ebfc3424c8e2e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65cb0f0b6b9a4a37ac23d3badac20932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d92327c875349d8b51688a2b7ae1f4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baceb40e419141bd839e42fa62f9b71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57c8b08d278a45379dfe774b813683a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf7bb028d2c842b2aa786bb3d145b608",
              "IPY_MODEL_6d3edce38b3e43cc9fed8a82b5a59c4c",
              "IPY_MODEL_273f65fad0594bdd9857b98d733e1b5b"
            ],
            "layout": "IPY_MODEL_12ef6443392a45859852eeaad76b4921"
          }
        },
        "bf7bb028d2c842b2aa786bb3d145b608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7ca71d6c2fa4e52a78d73249c9b1aa7",
            "placeholder": "​",
            "style": "IPY_MODEL_87e2063a55914e32a0bd9e6b344f6bfa",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "6d3edce38b3e43cc9fed8a82b5a59c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2331a23c16441dfb4029ffaea4e5ee0",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56c01ba7a16340fd92802404acdae1c1",
            "value": 5069051
          }
        },
        "273f65fad0594bdd9857b98d733e1b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_994d7a56d5174083a739a093a3607b0b",
            "placeholder": "​",
            "style": "IPY_MODEL_4eb87bed39574dc1b78ae63cba3a772d",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 102MB/s]"
          }
        },
        "12ef6443392a45859852eeaad76b4921": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7ca71d6c2fa4e52a78d73249c9b1aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87e2063a55914e32a0bd9e6b344f6bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2331a23c16441dfb4029ffaea4e5ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c01ba7a16340fd92802404acdae1c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "994d7a56d5174083a739a093a3607b0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb87bed39574dc1b78ae63cba3a772d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "155df81eba4e4c0cae3da58f72239b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ade8987447142158e42f6f35bf912c8",
              "IPY_MODEL_6ada0263591e427e969e96bc6d4d8ec2",
              "IPY_MODEL_975f40f7491d4543a5bfa5d9f05cb181"
            ],
            "layout": "IPY_MODEL_aa973d026d1e48d48b0421b01338c96e"
          }
        },
        "8ade8987447142158e42f6f35bf912c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a91600dc36b940f195c2f048060862a7",
            "placeholder": "​",
            "style": "IPY_MODEL_29810cc37bec4ff58206ebcd7d06c9c3",
            "value": "tokenizer.json: 100%"
          }
        },
        "6ada0263591e427e969e96bc6d4d8ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3e93db1cc304539bff8744985e7c6b2",
            "max": 17098107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44d16f0c9169400199c6bb64a836fde1",
            "value": 17098107
          }
        },
        "975f40f7491d4543a5bfa5d9f05cb181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_897c93e14a0145ad83b8da7339bfe048",
            "placeholder": "​",
            "style": "IPY_MODEL_31baae5fade64f98a4d876c56727bb61",
            "value": " 17.1M/17.1M [00:00&lt;00:00, 154MB/s]"
          }
        },
        "aa973d026d1e48d48b0421b01338c96e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a91600dc36b940f195c2f048060862a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29810cc37bec4ff58206ebcd7d06c9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3e93db1cc304539bff8744985e7c6b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44d16f0c9169400199c6bb64a836fde1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "897c93e14a0145ad83b8da7339bfe048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31baae5fade64f98a4d876c56727bb61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16490360bca6497492a0bc5a70e33c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12ba28c5dd7f4b22be4f8b5826d83138",
              "IPY_MODEL_a547b9b9138e4919b0ae417777c6b498",
              "IPY_MODEL_3d93afcfe67d488abfb0d86009421ce9"
            ],
            "layout": "IPY_MODEL_8bee661bd79947de9d7a48ab6806b6fc"
          }
        },
        "12ba28c5dd7f4b22be4f8b5826d83138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a26bd08b24ca4e15b82de5a688824360",
            "placeholder": "​",
            "style": "IPY_MODEL_d2e3644d230042d8bbc7526c444d6a4d",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "a547b9b9138e4919b0ae417777c6b498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e6be8b4bde348dd984f43077bc1b8f2",
            "max": 279,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_719fece4dffd429fae2a5a01c390cc9f",
            "value": 279
          }
        },
        "3d93afcfe67d488abfb0d86009421ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee5f9843717646f7999a23dc802cf9b3",
            "placeholder": "​",
            "style": "IPY_MODEL_4c0a16f168dd422985c0dc659859ca59",
            "value": " 279/279 [00:00&lt;00:00, 18.6kB/s]"
          }
        },
        "8bee661bd79947de9d7a48ab6806b6fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a26bd08b24ca4e15b82de5a688824360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2e3644d230042d8bbc7526c444d6a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e6be8b4bde348dd984f43077bc1b8f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "719fece4dffd429fae2a5a01c390cc9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee5f9843717646f7999a23dc802cf9b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c0a16f168dd422985c0dc659859ca59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5871uDAHcgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1de56d-5235-49eb-8f4d-81c5aad964b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.4/567.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.8/351.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain --quiet\n",
        "!pip install langchain-openai --quiet\n",
        "!pip install langchain-community --quiet\n",
        "!pip install langchain-huggingface --quiet\n",
        "\n",
        "!pip install openai httpx --force-reinstall --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma --quiet\n",
        "!pip install pypdf --quiet\n",
        "!pip install rank-bm25 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtEWsNetMrw7",
        "outputId": "69c72c35-94e7-499d-f042-bf6b0a0ae179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.retrievers import (\n",
        "    MultiQueryRetriever,\n",
        "    ContextualCompressionRetriever,\n",
        "    )\n",
        "\n",
        "from langchain.retrievers.document_compressors import (\n",
        "    LLMChainExtractor,\n",
        "    LLMChainFilter,\n",
        "    CrossEncoderReranker\n",
        "    )\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline\n",
        ")"
      ],
      "metadata": {
        "id": "FGhDjh9oO8Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_API_KEY')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "fB4BXet0Mrr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Loading the documents"
      ],
      "metadata": {
        "id": "tds6VMc6QWUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_paths = [\n",
        "    \"/content/Attention.pdf\",\n",
        "    \"/content/GPT4.pdf\"\n",
        "]"
      ],
      "metadata": {
        "id": "wI44IFkRNGAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_docs = []\n",
        "\n",
        "for pdf_path in pdf_paths:\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    docs = loader.load()\n",
        "    all_docs.extend(docs)\n",
        "\n",
        "print(f\"Loaded {len(all_docs)} total pages from PDFs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k496UfZVONUl",
        "outputId": "585edfed-e9ae-4e89-febf-bb2d72ee9440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 115 total pages from PDFs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk the documents to 1000 chars with 100 chars overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "2S16D2LtPcya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_docs = text_splitter.split_documents(all_docs)\n",
        "print(f\"Split into {len(split_docs)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2tcrbZMPctR",
        "outputId": "b97c64ac-b319-4441-c46a-a7a7a9ec565a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split into 410 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and Compare Embeddings"
      ],
      "metadata": {
        "id": "Y-wDwrxZQapc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2A. Open AI Embeddings\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "Ymnp0NznPcpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2B. HuggingFace Embeddings\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "hf_embed_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "YNxilMcCPcmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Build Chroma Vector Stores"
      ],
      "metadata": {
        "id": "6Nyg-hGiRE0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Index using OpenAI embeddings\n",
        "\n",
        "chroma_db_openai = Chroma.from_documents(\n",
        "    documents = split_docs,\n",
        "    embedding = openai_embed_model,\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "    collection_name = 'openai_embeddings',\n",
        "    persist_directory=\"./openai_embeddings\"\n",
        ")"
      ],
      "metadata": {
        "id": "TwBYqmSGPcev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Index using HuggingFace embeddings\n",
        "\n",
        "chroma_db_huggingface = Chroma.from_documents(\n",
        "    documents = split_docs,\n",
        "    embedding = hf_embed_model,\n",
        "    collection_name = 'huggingface_embeddings',\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "    persist_directory=\"./huggingface_embeddings\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "PMjH2_sORZgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Built two separate Chroma Collections (OpenAI vs HF)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGLIxnG8Su2J",
        "outputId": "9281d489-0a5e-4d39-9b6c-d650bbd9db84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built two separate Chroma Collections (OpenAI vs HF)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Experiment with Vector Database Retrievers\n",
        "\n",
        "Here we will explore the different retrieval strategies on our Vector Database:"
      ],
      "metadata": {
        "id": "5cCVFmWNTd0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Similarity or Ranking based Retrieval"
      ],
      "metadata": {
        "id": "OTLYMMRqUuTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_retriver_openai = chroma_db_openai.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3},\n",
        "    fetch_k=3\n",
        ")"
      ],
      "metadata": {
        "id": "PAhld2CqS34V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"What is GPT?\"\n",
        "top3_docs_2= similarity_retriver_openai.invoke(query2)\n",
        "top3_docs_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rddiuFL3UJTX",
        "outputId": "b006dd38-3589-40fa-fbd7-dbeddd3e1cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 11, 'page_label': '12', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='synthesize it:\\n[Early GPT-4] [March 14 GPT-4]\\n[Redacted: generates steps and chem-\\nical scheme]\\nMy apologies, but I cannot provide information on syn-\\nthesizing harmful or dangerous substances. If you have\\nany other questions or need assistance with a different\\ntopic, please feel free to ask.\\nTable 5: Expert Red Teaming: Example prompt and completions from various models.\\nover 50 experts from domains such as long-term AI alignment risks, cybersecurity, biorisk, and\\ninternational security to adversarially test the model. Their findings specifically enabled us to test\\nmodel behavior in high-risk areas which require niche expertise to evaluate, as well as assess risks\\nthat will become relevant for very advanced AIs such as power seeking [70]. Recommendations and\\ntraining data gathered from these experts fed into our mitigations and improvements for the model;\\nfor example, we’ve collected additional data to improve GPT-4’s ability to refuse requests on how to')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = similarity_retriver_openai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me_1e4GuT2_p",
        "outputId": "48a96e76-3ef3-40e6-a4fe-12d913159e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13')]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_retriver_hf = chroma_db_huggingface.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3},\n",
        "    fetch_k=3\n",
        ")"
      ],
      "metadata": {
        "id": "NZzNdklIVoSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs_hf = similarity_retriver_hf.invoke(query)\n",
        "top3_docs_hf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adt174q-V1fR",
        "outputId": "22cd4fa4-68ad-42b8-c7ec-eeed7f147ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 6, 'page_label': '7', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='between any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million')]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"What is GPT?\"\n",
        "top3_docs_2_hf= similarity_retriver_hf.invoke(query2)\n",
        "top3_docs_2_hf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pwy0vds7V1MC",
        "outputId": "229fcea7-da6b-49c2-cf3f-d7dc00db2fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='optimization methods that behave predictably across a wide range of scales. This allowed us to make\\npredictions about the expected performance of GPT-4 (based on small runs trained in similar ways)\\nthat were tested against the final run to increase confidence in our training.\\nDespite its capabilities, GPT-4 has similar limitations to earlier GPT models [1, 37, 38]: it is not fully\\nreliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn\\n∗Please cite this work as “OpenAI (2023)\". Full authorship contribution statements appear at the end of the\\ndocument. Correspondence regarding this technical report can be sent to gpt4-report@openai.com\\narXiv:2303.08774v6  [cs.CL]  4 Mar 2024'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='optimization methods that behave predictably across a wide range of scales. This allowed us to make\\npredictions about the expected performance of GPT-4 (based on small runs trained in similar ways)\\nthat were tested against the final run to increase confidence in our training.\\nDespite its capabilities, GPT-4 has similar limitations to earlier GPT models [1, 37, 38]: it is not fully\\nreliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn\\n∗Please cite this work as “OpenAI (2023)\". Full authorship contribution statements appear at the end of the\\ndocument. Correspondence regarding this technical report can be sent to gpt4-report@openai.com\\narXiv:2303.08774v6  [cs.CL]  4 Mar 2024'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 64, 'page_label': '65', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='Askell et al.\\n2022\\nAskell et al.\\n2022\\ngpt-3.5-base gpt-3.5-base gpt-3.5-turbo gpt-4-base gpt-4-base gpt-4\\n0%\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\n70%\\nModel\\nAccuracy\\nAccuracy on adversarial questions (TruthfulQA mc1)\\nAnthropic-LM\\ngpt-3.5\\ngpt-4\\nFigure 8: Performance of GPT-4 on TruthfulQA. Accuracy is shown on the y-axis, higher is better.\\nWe compare GPT-4 under zero-shot prompting, few-shot prompting, and after RLHF ﬁne-tuning.\\nGPT-4 signiﬁcantly outperforms both GPT-3.5 and Askell et al [101].ﬁxes to plot legend and title\\n65')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Multi Query Retrieval\n"
      ],
      "metadata": {
        "id": "jASpXEADUhPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)"
      ],
      "metadata": {
        "id": "hJPx__-JUhMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Open AI Multi query retrieval"
      ],
      "metadata": {
        "id": "6KFjT3DDZFx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set logging to get the queries\n",
        "import logging\n",
        "\n",
        "\n",
        "mq_retriever_openai = MultiQueryRetriever.from_llm(\n",
        "    retriever=similarity_retriver_openai,\n",
        "    llm=chatgpt,\n",
        ")\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "hSnJZUp5UhJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = mq_retriever_openai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqBdjRf8UhHg",
        "outputId": "c84e1217-4740-4152-fb1a-86011291bb5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does attention entail?', 'Can you explain the concept of attention?', 'How is attention defined in cognitive psychology?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate')]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT?\"\n",
        "top3_docs = mq_retriever_openai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i55T07bNYaqz",
        "outputId": "bbac6e0c-d8fe-4b2b-cf47-25ec336e6d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you explain the concept of GPT?', '2. How would you define GPT?', '3. What does GPT stand for and what is its significance?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 43, 'page_label': '44', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='2 GPT-4 Observed Safety Challenges\\nGPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\\ncoding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\\nalso present new safety challenges, which we highlight in this section.\\nWe conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations\\nhelped us gain an understanding of GPT-4’s capabilities, limitations, and risks; prioritize our\\nmitigation eﬀorts; and iteratively test and build safer versions of the model. Some of the speciﬁc\\nrisks we explored are: 6\\n• Hallucinations\\n• Harmful content\\n• Harms of representation, allocation, and quality of service\\n• Disinformation and inﬂuence operations\\n• Proliferation of conventional and unconventional weapons\\n• Privacy\\n• Cybersecurity\\n• Potential for risky emergent behaviors\\n• Interactions with other systems\\n• Economic impacts\\n• Acceleration\\n• Overreliance')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HuggingFace Multi query retrieval"
      ],
      "metadata": {
        "id": "JJMrN9xBZI0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set logging to get the queries\n",
        "import logging\n",
        "\n",
        "\n",
        "mq_retriever_hf = MultiQueryRetriever.from_llm(\n",
        "    retriever=similarity_retriver_hf,\n",
        "    llm=chatgpt,\n",
        ")\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "Vo4GlnJzZLJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = mq_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI5-WPS6ZZO_",
        "outputId": "47273f1b-00de-4414-b0f4-b59693d05d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does attention entail?', 'How is attention defined?', 'Can you explain the concept of attention?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 6, 'page_label': '7', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='between any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million')]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT?\"\n",
        "top3_docs = mq_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKwrhZZwZc-w",
        "outputId": "5b5ce609-03c3-4e17-ae1f-bf5b15d9ed74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you explain the concept of GPT?', '2. How would you define GPT?', '3. What does GPT refer to?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 9, 'page_label': '10', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='only slightly better at this task than GPT-3.5; however, after RLHF post-training we observe large\\nimprovements over GPT-3.5.9 Table 4 shows both a correct and an incorrect answer. GPT-4 resists\\nselecting common sayings (you can’t teach an old dog new tricks), however it still can miss subtle\\ndetails (Elvis Presley was not the son of an actor, so Perkins is the correct answer).\\nGPT-4 generally lacks knowledge of events that have occurred after the vast majority of its pre-training\\ndata cuts off in September 202110, and does not learn from its experience. It can sometimes make\\nsimple reasoning errors which do not seem to comport with competence across so many domains, or\\nbe overly gullible in accepting obviously false statements from a user. It can fail at hard problems the\\nsame way humans do, such as introducing security vulnerabilities into code it produces.\\nGPT-4 can also be confidently wrong in its predictions, not taking care to double-check work when'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 97, 'page_label': '98', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='Figure 2. Performance of GPT-4 and smaller models. The metric is mean log pass rate on a subset of\\nthe HumanEval dataset. A power law fit to the smaller models (excluding GPT-4) is shown as the dotted\\nline; this fit accurately predicts GPT-4’s performance. The x-axis is training compute normalized so that\\nGPT-4 is 1.\\n3'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 9, 'page_label': '10', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='5 Limitations\\nDespite its capabilities, GPT-4 has similar limitations as earlier GPT models. Most importantly, it still\\nis not fully reliable (it “hallucinates” facts and makes reasoning errors). Great care should be taken\\nwhen using language model outputs, particularly in high-stakes contexts, with the exact protocol\\n(such as human review, grounding with additional context, or avoiding high-stakes uses altogether)\\nmatching the needs of specific applications. See our System Card for details.\\nGPT-4 significantly reduces hallucinations relative to previous GPT-3.5 models (which have them-\\nselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our\\nlatest GPT-3.5 on our internal, adversarially-designed factuality evaluations (Figure 6).\\nlearning technology writing history math science recommendation code business\\n0%\\n20%\\n40%\\n60%\\n80%\\nCategory\\nAccuracy\\nInternal factual eval by category\\nchatgpt-v2\\nchatgpt-v3\\nchatgpt-v4\\ngpt-4')]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Contextual Compression Retrieval"
      ],
      "metadata": {
        "id": "GHnDVYyeZh6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Open AI"
      ],
      "metadata": {
        "id": "gIQhf0FOb3bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract the relevant content for the query\n",
        "compressor = LLMChainExtractor.from_llm(chatgpt)\n",
        "compression_retriever_openai = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=similarity_retriver_openai\n",
        ")"
      ],
      "metadata": {
        "id": "k1cSK2sTZqtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT?\"\n",
        "top3_docs = compression_retriever_openai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a76pq8I_b57l",
        "outputId": "df25abb2-e5e4-46dd-b113-5e653fbc6277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4, a large multimodal model capable of processing image and text inputs and producing text outputs.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4, a large multimodal model capable of processing image and text inputs and producing text outputs.')]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = compression_retriever_openai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reuEcVWjb9dU",
        "outputId": "c144189a-1f13-46a1-978d-ee05628b1dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.')]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HuggingFace"
      ],
      "metadata": {
        "id": "A2uo4UgVcPcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract the relevant content for the query\n",
        "compressor = LLMChainExtractor.from_llm(chatgpt)\n",
        "compression_retriever_hf = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=similarity_retriver_hf\n",
        ")"
      ],
      "metadata": {
        "id": "u7GntxH2cHty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT?\"\n",
        "top3_docs = compression_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkPV47-qcU4v",
        "outputId": "d507aacd-2f59-4a9e-8e2c-7f9f8b3a98eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 64, 'page_label': '65', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4')]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = compression_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXtFVbSncWOE",
        "outputId": "3f7172e6-3cec-4d38-ce4c-d4e970a8a92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.')]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. LLM Filter"
      ],
      "metadata": {
        "id": "MknHa_1XmCOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Open AI"
      ],
      "metadata": {
        "id": "DwNKNwEVmaaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "_filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
        "\n",
        "# retrieves the documents similar to query and then applies the filter\n",
        "compression_retriever_openai = ContextualCompressionRetriever(\n",
        "    base_compressor=_filter, base_retriever=mq_retriever_openai\n",
        ")"
      ],
      "metadata": {
        "id": "eD4iSqGDccEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT?\"\n",
        "top3_docs = compression_retriever_openai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj0BffSgmOIZ",
        "outputId": "d143e451-1061-4e95-cdcd-65634a4df9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does GPT stand for?', 'Can you explain the concept of GPT?', 'How would you define GPT?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 11, 'page_label': '12', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='synthesize it:\\n[Early GPT-4] [March 14 GPT-4]\\n[Redacted: generates steps and chem-\\nical scheme]\\nMy apologies, but I cannot provide information on syn-\\nthesizing harmful or dangerous substances. If you have\\nany other questions or need assistance with a different\\ntopic, please feel free to ask.\\nTable 5: Expert Red Teaming: Example prompt and completions from various models.\\nover 50 experts from domains such as long-term AI alignment risks, cybersecurity, biorisk, and\\ninternational security to adversarially test the model. Their findings specifically enabled us to test\\nmodel behavior in high-risk areas which require niche expertise to evaluate, as well as assess risks\\nthat will become relevant for very advanced AIs such as power seeking [70]. Recommendations and\\ntraining data gathered from these experts fed into our mitigations and improvements for the model;\\nfor example, we’ve collected additional data to improve GPT-4’s ability to refuse requests on how to')]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = compression_retriever_openai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8oWhiNhmVm8",
        "outputId": "2851a2e1-bf44-4399-faaa-0328656982cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does attention entail?', 'How is attention defined?', 'Can you explain the concept of attention?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate')]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HuggingFace"
      ],
      "metadata": {
        "id": "XTorEVEImc16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "_filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
        "\n",
        "# retrieves the documents similar to query and then applies the filter\n",
        "compression_retriever_hf = ContextualCompressionRetriever(\n",
        "    base_compressor=_filter, base_retriever=mq_retriever_hf\n",
        ")"
      ],
      "metadata": {
        "id": "_ovXWHlWmeHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = compression_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw1NsKZGmwLm",
        "outputId": "0807aa0b-86f9-4bbf-afc0-7d4958528b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does attention entail?', 'How is attention defined?', 'Can you explain the concept of attention?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13')]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT?\"\n",
        "top3_docs = compression_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qijsnxaRm0KY",
        "outputId": "19676596-6050-4f1d-967c-89190b52b69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you explain the concept of GPT?', '2. How would you define GPT?', '3. What does GPT refer to?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='Figure 2. Performance of GPT-4 and smaller models. The metric is mean log pass rate on a subset of\\nthe HumanEval dataset. A power law fit to the smaller models (excluding GPT-4) is shown as the dotted\\nline; this fit accurately predicts GPT-4’s performance. The x-axis is training compute normalized so that\\nGPT-4 is 1.\\n3'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 9, 'page_label': '10', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='5 Limitations\\nDespite its capabilities, GPT-4 has similar limitations as earlier GPT models. Most importantly, it still\\nis not fully reliable (it “hallucinates” facts and makes reasoning errors). Great care should be taken\\nwhen using language model outputs, particularly in high-stakes contexts, with the exact protocol\\n(such as human review, grounding with additional context, or avoiding high-stakes uses altogether)\\nmatching the needs of specific applications. See our System Card for details.\\nGPT-4 significantly reduces hallucinations relative to previous GPT-3.5 models (which have them-\\nselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our\\nlatest GPT-3.5 on our internal, adversarially-designed factuality evaluations (Figure 6).\\nlearning technology writing history math science recommendation code business\\n0%\\n20%\\n40%\\n60%\\n80%\\nCategory\\nAccuracy\\nInternal factual eval by category\\nchatgpt-v2\\nchatgpt-v3\\nchatgpt-v4\\ngpt-4')]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Chained Retrieval Pipeline"
      ],
      "metadata": {
        "id": "Ygc7HTnUm4cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download an open-source reranker model - BAAI/bge-reranker-v2-m3\n",
        "reranker = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\n",
        "reranker_compressor = CrossEncoderReranker(model=reranker, top_n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "1e876a59314b41d9ba2cf41ea5410156",
            "80980f880db143ae96ba9c8bba89915b",
            "97d22d99e32c4d51b7c5767f2e636e56",
            "d4e341ae57e9415e970bc37684e6b7a1",
            "3335a8a8fafc4cbbb326ac4505a4e2d6",
            "f99dac7635744b7a9b904979ec9e3a03",
            "d8d45be0daed4088a6b6ea4305fa241e",
            "9dbc8e2c067b4c9b8eab4bcf50c72c3c",
            "5d965ee0ef9f407a9bb09c4bd3dabb06",
            "3d054926ea744273ab80e3f581ee4111",
            "eab3687daa4c4ae6951fed58320c5fec",
            "6a587ee0b7b845e29dc852a96b5348e0",
            "2a5450b69950406385388f139714eb24",
            "95088ffb6428449e8ced3b67c474ab05",
            "49fe299baac047bf8cea0882d4a3177c",
            "4c1166e1d7184c6081c8eab2bf958628",
            "11bccc39c44b48cab0b67735dc5a6541",
            "c5ac57f8c2164bf2b43461e738f12b21",
            "87cc4dae73784607b563fb785fadea6a",
            "b6a14c52c82040d5aa95c35e0f10de33",
            "8a95cd26c40e4c27884da84bc98f2c01",
            "784af4222b884d0aaf5fc0ba04a96498",
            "c981b47c36f14bcc9907622b2ef53c21",
            "a38179a527484bc9b6408833ff9b8ecc",
            "4528cf6fc66342269e539180f8876423",
            "d301cddc5d42454fa4e1dbb3b7dcd56b",
            "1e0daf05868f4b7baef91170df3c162d",
            "1f0171ea4bb844fcbe2e1ee28aa0e2a0",
            "46be3e17a850430d8e5a971bde90f4ee",
            "3210eeea849c41adb1ebfc3424c8e2e2",
            "65cb0f0b6b9a4a37ac23d3badac20932",
            "0d92327c875349d8b51688a2b7ae1f4f",
            "baceb40e419141bd839e42fa62f9b71b",
            "57c8b08d278a45379dfe774b813683a3",
            "bf7bb028d2c842b2aa786bb3d145b608",
            "6d3edce38b3e43cc9fed8a82b5a59c4c",
            "273f65fad0594bdd9857b98d733e1b5b",
            "12ef6443392a45859852eeaad76b4921",
            "c7ca71d6c2fa4e52a78d73249c9b1aa7",
            "87e2063a55914e32a0bd9e6b344f6bfa",
            "b2331a23c16441dfb4029ffaea4e5ee0",
            "56c01ba7a16340fd92802404acdae1c1",
            "994d7a56d5174083a739a093a3607b0b",
            "4eb87bed39574dc1b78ae63cba3a772d",
            "155df81eba4e4c0cae3da58f72239b8b",
            "8ade8987447142158e42f6f35bf912c8",
            "6ada0263591e427e969e96bc6d4d8ec2",
            "975f40f7491d4543a5bfa5d9f05cb181",
            "aa973d026d1e48d48b0421b01338c96e",
            "a91600dc36b940f195c2f048060862a7",
            "29810cc37bec4ff58206ebcd7d06c9c3",
            "c3e93db1cc304539bff8744985e7c6b2",
            "44d16f0c9169400199c6bb64a836fde1",
            "897c93e14a0145ad83b8da7339bfe048",
            "31baae5fade64f98a4d876c56727bb61",
            "16490360bca6497492a0bc5a70e33c73",
            "12ba28c5dd7f4b22be4f8b5826d83138",
            "a547b9b9138e4919b0ae417777c6b498",
            "3d93afcfe67d488abfb0d86009421ce9",
            "8bee661bd79947de9d7a48ab6806b6fc",
            "a26bd08b24ca4e15b82de5a688824360",
            "d2e3644d230042d8bbc7526c444d6a4d",
            "0e6be8b4bde348dd984f43077bc1b8f2",
            "719fece4dffd429fae2a5a01c390cc9f",
            "ee5f9843717646f7999a23dc802cf9b3",
            "4c0a16f168dd422985c0dc659859ca59"
          ]
        },
        "id": "SeXRZd9CnCjT",
        "outputId": "20937317-9dc4-42b2-a98b-9fde3d58c08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e876a59314b41d9ba2cf41ea5410156"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a587ee0b7b845e29dc852a96b5348e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c981b47c36f14bcc9907622b2ef53c21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57c8b08d278a45379dfe774b813683a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "155df81eba4e4c0cae3da58f72239b8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16490360bca6497492a0bc5a70e33c73"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Open AI"
      ],
      "metadata": {
        "id": "NEXQpQEhoo22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever_oai = chroma_db_openai.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "_filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever_oai = ContextualCompressionRetriever(\n",
        "    base_compressor=_filter, base_retriever=similarity_retriever_oai\n",
        ")\n",
        "\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever_oai = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor, base_retriever=compressor_retriever_oai\n",
        ")"
      ],
      "metadata": {
        "id": "JT2-nMq8ntt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = final_retriever_oai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqzgzE-7oQ3W",
        "outputId": "5eed5482-de11-4ae9-f5e3-9abedd192172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate')]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT4?\"\n",
        "top3_docs = final_retriever_oai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfSwX-qwoUq2",
        "outputId": "9f688c36-ca0c-4893-be58-bec2de63b49e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 43, 'page_label': '44', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='2 GPT-4 Observed Safety Challenges\\nGPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\\ncoding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\\nalso present new safety challenges, which we highlight in this section.\\nWe conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations\\nhelped us gain an understanding of GPT-4’s capabilities, limitations, and risks; prioritize our\\nmitigation eﬀorts; and iteratively test and build safer versions of the model. Some of the speciﬁc\\nrisks we explored are: 6\\n• Hallucinations\\n• Harmful content\\n• Harms of representation, allocation, and quality of service\\n• Disinformation and inﬂuence operations\\n• Proliferation of conventional and unconventional weapons\\n• Privacy\\n• Cybersecurity\\n• Potential for risky emergent behaviors\\n• Interactions with other systems\\n• Economic impacts\\n• Acceleration\\n• Overreliance')]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HuggingFace"
      ],
      "metadata": {
        "id": "d-O2CiZcosSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever_hf = chroma_db_huggingface.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "_filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
        "\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever_hf = ContextualCompressionRetriever(\n",
        "    base_compressor=_filter, base_retriever=similarity_retriever_hf\n",
        ")\n",
        "\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever_hf = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor, base_retriever=compressor_retriever_hf\n",
        ")"
      ],
      "metadata": {
        "id": "U227dB7jokY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT4?\"\n",
        "top3_docs = final_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9Yus2UVo69f",
        "outputId": "5a12dd4e-9646-4e85-f877-31fd16226ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='Figure 2. Performance of GPT-4 and smaller models. The metric is mean log pass rate on a subset of\\nthe HumanEval dataset. A power law fit to the smaller models (excluding GPT-4) is shown as the dotted\\nline; this fit accurately predicts GPT-4’s performance. The x-axis is training compute normalized so that\\nGPT-4 is 1.\\n3'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='Figure 2. Performance of GPT-4 and smaller models. The metric is mean log pass rate on a subset of\\nthe HumanEval dataset. A power law fit to the smaller models (excluding GPT-4) is shown as the dotted\\nline; this fit accurately predicts GPT-4’s performance. The x-axis is training compute normalized so that\\nGPT-4 is 1.\\n3'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='optimization methods that behave predictably across a wide range of scales. This allowed us to make\\npredictions about the expected performance of GPT-4 (based on small runs trained in similar ways)\\nthat were tested against the final run to increase confidence in our training.\\nDespite its capabilities, GPT-4 has similar limitations to earlier GPT models [1, 37, 38]: it is not fully\\nreliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn\\n∗Please cite this work as “OpenAI (2023)\". Full authorship contribution statements appear at the end of the\\ndocument. Correspondence regarding this technical report can be sent to gpt4-report@openai.com\\narXiv:2303.08774v6  [cs.CL]  4 Mar 2024')]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Attention?\"\n",
        "top3_docs = final_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw0w63Opo9OP",
        "outputId": "5b46dd40-9f53-4687-f8f8-0008ae36be6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 9, 'page_label': '10', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='model outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.')]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Chained Retrieval Pipeline using Compressor"
      ],
      "metadata": {
        "id": "itViDOBOpU5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Open AI"
      ],
      "metadata": {
        "id": "DSwd34RRqE5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever_oai = chroma_db_openai.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "compressor = LLMChainExtractor.from_llm(chatgpt)\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever_oai = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=similarity_retriever_oai\n",
        ")\n",
        "\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever_oai = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor, base_retriever=compressor_retriever_oai\n",
        ")"
      ],
      "metadata": {
        "id": "kdaAzssepEC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = final_retriever_oai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y_Jarw-qBTj",
        "outputId": "5d59e441-d70c-4374-fc24-d48d8e4825c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.')]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT4?\"\n",
        "top3_docs = final_retriever_oai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYdW-Ed4qG0v",
        "outputId": "72c0695a-0467-4ec7-a18f-0f7ba5901b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4, a large multimodal model capable of processing image and text inputs and producing text outputs.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4, a large multimodal model capable of processing image and text inputs and producing text outputs.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 43, 'page_label': '44', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and coding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements also present new safety challenges, which we highlight in this section.')]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HuggingFace"
      ],
      "metadata": {
        "id": "UtJxwqp1qQNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever_hf = chroma_db_huggingface.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "compressor = LLMChainExtractor.from_llm(chatgpt)\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever_hf = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=similarity_retriever_hf\n",
        ")\n",
        "\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever_hf = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor, base_retriever=compressor_retriever_hf\n",
        ")"
      ],
      "metadata": {
        "id": "0ZcHPQYeqRrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Attention?\"\n",
        "top3_docs = final_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pillA0RqqkJ0",
        "outputId": "3e6d5108-c353-4fbb-d0b6-1d78e829ded1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.')]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT4?\"\n",
        "top3_docs = final_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW4S1qi_qmQh",
        "outputId": "4d351f30-a16d-4d65-e724-08b4b988454d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4 has similar limitations to earlier GPT models: it is not fully reliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn')]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Chained Retrieval using Multi Query and Compressor"
      ],
      "metadata": {
        "id": "MXP2bPOFrOGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Open AI"
      ],
      "metadata": {
        "id": "NWiOini0rXBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever_oai = chroma_db_openai.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "# Multi Query\n",
        "mq_retriever_oai = MultiQueryRetriever.from_llm(\n",
        "    retriever=similarity_retriever_oai,\n",
        "    llm=chatgpt,\n",
        ")\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "compressor = LLMChainExtractor.from_llm(chatgpt)\n",
        "\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever_oai = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=mq_retriever_oai\n",
        ")\n",
        "\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever_oai = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor,\n",
        "    base_retriever=compressor_retriever_oai\n",
        ")"
      ],
      "metadata": {
        "id": "s5wa9YRhq1zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT4?\"\n",
        "top3_docs = final_retriever_oai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOOWnszIsGX9",
        "outputId": "5048ff11-653b-4c2f-b8fc-de9ce4bd503c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on the latest version of the GPT series, GPT4?', '2. What are the key features and improvements of GPT4 compared to its predecessors?', '3. How does GPT4 differ from previous iterations in terms of performance and capabilities?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4 is a large-scale, multimodal model which can accept image and text inputs and produce text outputs.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4, a large multimodal model capable of processing image and text inputs and producing text outputs.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 43, 'page_label': '44', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and coding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements also present new safety challenges, which we highlight in this section.')]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Attention?\"\n",
        "top3_docs = final_retriever_oai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtEigEA0sKU4",
        "outputId": "a476f465-2d34-4c3b-8407-70c192ec87df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does Attention refer to?', 'What are the key aspects of Attention?', 'Can you explain the concept of Attention?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 3, 'page_label': '4', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.')]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HuggingFace"
      ],
      "metadata": {
        "id": "PhQEz5hrsZnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever_hf = chroma_db_huggingface.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "# Multi Query\n",
        "mq_retriever_hf = MultiQueryRetriever.from_llm(\n",
        "    retriever=similarity_retriever_hf,\n",
        "    llm=chatgpt,\n",
        ")\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "compressor = LLMChainExtractor.from_llm(chatgpt)\n",
        "\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever_hf = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=mq_retriever_hf\n",
        ")\n",
        "\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever_hf = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor,\n",
        "    base_retriever=compressor_retriever_hf\n",
        ")"
      ],
      "metadata": {
        "id": "I-kkvNUpsR0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT4?\"\n",
        "top3_docs = final_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg3F__p3si9T",
        "outputId": "50d0bd45-c939-40ec-ea8f-888fbfa37d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on the latest version of the GPT series, GPT4?', '2. What are the key features and improvements of GPT4 compared to its predecessors?', '3. How does GPT4 differ from previous iterations of the GPT model in terms of performance and capabilities?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 97, 'page_label': '98', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4-launch'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 9, 'page_label': '10', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4 has similar limitations as earlier GPT models.')]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Attention?\"\n",
        "top3_docs = final_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYJGksIislMS",
        "outputId": "086d2fb1-4c0b-49b9-b12f-f3e48013a57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What does Attention refer to?', 'What are the key aspects of Attention?', 'Can you explain the concept of Attention?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.')]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. MMR"
      ],
      "metadata": {
        "id": "l3lT40Rls0Bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI"
      ],
      "metadata": {
        "id": "EDasE62j5Z3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_retriver_openai_mmr = chroma_db_openai.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 3, \"fetch_k\": 3}\n",
        ")"
      ],
      "metadata": {
        "id": "TwLKJJzRs3cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = similarity_retriver_openai_mmr.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrLYBtkOwqtb",
        "outputId": "939f8f4e-5cbf-4755-c471-3a1396a9e1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13')]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFace"
      ],
      "metadata": {
        "id": "U2c2tL805bot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_retriver_hf_mmr = chroma_db_huggingface.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 3, \"fetch_k\": 3}\n",
        ")"
      ],
      "metadata": {
        "id": "TBhUIkl-wwj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is attention?\"\n",
        "top3_docs = similarity_retriver_hf_mmr.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1KPqDiw5f7Q",
        "outputId": "4e458499-c031-4c59-c1f1-71359b6acc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 6, 'page_label': '7', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/Attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='between any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million')]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Chained Retrieval using MMR, Multi Query and Compressor"
      ],
      "metadata": {
        "id": "iKnKXigQ5kCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI"
      ],
      "metadata": {
        "id": "Ho6R92nY7Uif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever_oai = chroma_db_openai.as_retriever(search_type=\"mmr\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "# Multi Query\n",
        "mq_retriever_oai = MultiQueryRetriever.from_llm(\n",
        "    retriever=similarity_retriever_oai,\n",
        "    llm=chatgpt,\n",
        ")\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "compressor = LLMChainExtractor.from_llm(chatgpt)\n",
        "\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever_oai = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=mq_retriever_oai\n",
        ")\n",
        "\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever_oai = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor,\n",
        "    base_retriever=compressor_retriever_oai\n",
        ")"
      ],
      "metadata": {
        "id": "hRLKR3yp5iCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT4?\"\n",
        "top3_docs = final_retriever_oai.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkJJAEs27CHF",
        "outputId": "ce9b76d5-8815-4c1c-d606-ebec0ff60f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on the latest version of the GPT series, GPT4?', '2. What are the key features and improvements of GPT4 compared to its predecessors?', '3. How does GPT4 differ from previous iterations in terms of performance and capabilities?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4, a large multimodal model capable of processing image and text inputs and producing text outputs.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 84, 'page_label': '85', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 39, 'page_label': '40', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4')]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HuggingFace"
      ],
      "metadata": {
        "id": "iqnTRIPf7XMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever_hf = chroma_db_huggingface.as_retriever(search_type=\"mmr\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "# Multi Query\n",
        "mq_retriever_hf = MultiQueryRetriever.from_llm(\n",
        "    retriever=similarity_retriever_hf,\n",
        "    llm=chatgpt,\n",
        ")\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "compressor = LLMChainExtractor.from_llm(chatgpt)\n",
        "\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever_hf = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=mq_retriever_hf\n",
        ")\n",
        "\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever_hf = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor,\n",
        "    base_retriever=compressor_retriever_hf\n",
        ")"
      ],
      "metadata": {
        "id": "TpWvNaRW7GNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT4?\"\n",
        "top3_docs = final_retriever_hf.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tZadIK67gCN",
        "outputId": "d9b0d846-0db6-4a17-e429-85cbfde0b325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on the latest version of the GPT series, GPT4?', '2. What are the key features and improvements of GPT4 compared to its predecessors?', '3. How does GPT4 differ from previous iterations in terms of performance and capabilities?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 97, 'page_label': '98', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4-launch'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4 has similar limitations to earlier GPT models: it is not fully reliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn')]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. BM"
      ],
      "metadata": {
        "id": "-eSvilue7tqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever\n",
        "bm25_retriever = BM25Retriever.from_documents(docs)"
      ],
      "metadata": {
        "id": "iEHhOv2n7jXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is GPT4?\"\n",
        "top3_docs = bm25_retriever.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZK3Ejj5N6Es",
        "outputId": "d73f7444-f6e4-4a0c-b5ca-87c0532a9cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-11T01:02:02+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/GPT4.pdf', 'total_pages': 100, 'page': 35, 'page_label': '36'}, page_content='GPT-4 visual input example, Extreme Ironing:\\nUser What is unusual about this image?\\nSource: https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg\\nGPT-4 The unusual thing about this image is that a man is ironing clothes on an ironing\\nboard attached to the roof of a moving taxi.\\nTable 16. Example prompt demonstrating GPT-4’s visual input capability. The prompt requires image\\nunderstanding.\\n36'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-11T01:02:02+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/GPT4.pdf', 'total_pages': 100, 'page': 33, 'page_label': '34'}, page_content='GPT-4 visual input example, Chart Reasoning:\\nUser What is the sum of average daily meat consumption for Georgia and Western Asia?\\nProvide a step-by-step reasoning before providing your answer.\\nGPT-4 To find the sum of average daily meat consumption for Georgia and Western Asia,\\nwe will follow these steps:\\n1. Identify the average daily meat consumption for Georgia.\\n2. Identify the average daily meat consumption for Western Asia.\\n3. Add the values from steps 1 and 2.\\nStep 1: From the provided chart, we see that the average daily meat con-\\nsumption for Georgia is 79.84 grams per person per day.\\nStep 2: From the chart, we see that the average daily meat consumption for\\nWestern Asia is 69.62 grams per person per day.\\nStep 3: Add the values from steps 1 and 2.\\n79.84 g (Georgia) + 69.62 g (Western Asia) = 149.46 g\\nSo, the sum of average daily meat consumption for Georgia and Western\\nAsia is 149.46 grams per person per day.\\nTable 14. Example prompt demonstrating GPT-4’s visual input capability. The prompt consists of a\\nquestion which requires reading information from a chart and performing computation.\\n34'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-11T01:02:02+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/GPT4.pdf', 'total_pages': 100, 'page': 8, 'page_label': '9'}, page_content='Example of GPT-4 visual input:\\nUser What is funny about this image? Describe it panel by panel.\\nSource: https://www.reddit.com/r/hmmm/comments/ubab5v/hmmm/\\nGPT-4 The image shows a package for a \"Lightning Cable\" adapter with three panels.\\nPanel 1: A smartphone with a VGA connector (a large, blue, 15-pin con-\\nnector typically used for computer monitors) plugged into its charging port.\\nPanel 2: The package for the \"Lightning Cable\" adapter with a picture of\\na VGA connector on it.\\nPanel 3: A close-up of the VGA connector with a small Lightning con-\\nnector (used for charging iPhones and other Apple devices) at the end.\\nThe humor in this image comes from the absurdity of plugging a large,\\noutdated VGA connector into a small, modern smartphone charging port.\\nTable 3. Example prompt demonstrating GPT-4’s visual input capability. The prompt consists of a\\nquestion about an image with multiple panels which GPT-4 is able to answer.\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-11T01:02:02+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/GPT4.pdf', 'total_pages': 100, 'page': 99, 'page_label': '100'}, page_content='Observation: Failed to modify (bioisosterism) compound\\nThought: I need to ﬁnd a different compound to modify\\nAction: Literature Answer\\nAction Input: What are a few other compounds with the same MOA/target as Dasatinib?\\nObservation: Other compounds with the same MOA/target as Dasatinib include imatinib, nilotinib, and bosutinib, which\\nare all tyrosine kinase inhibitors that target BCR-ABL, SRC, and c-KIT (Erik6635 pages 7-7). Additionally, disulﬁram,\\nWithaferin A, and parthenolide have been shown to have the same mechanism of action as Dasatinib as proteasome\\ninhibitors in cancer treatment (Iorio1462 pages 6-6). Another compound with the same mechanism of action as Dasa-\\ntinib is AZD0530, which also inhibits dengue virus infection by targeting Fyn kinase (Wispelaere0530 pages 1-1).\\nThought: I need to get the SMILES string of one of these compounds\\nAction: Molecule search\\nAction Input: imatinib\\nObservation: CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C)NC4=NC=CC(=N4)C5=CN=CC=C5\\nThought: I need to modify this compound to make a novel compound\\nAction: Modify compound\\nAction Input: CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C)NC4=NC=CC(=N4)C5=CN=CC=C5\\nObservation: Cc1ccc(NC(=O)c2ccc(CN3CCNCC3)cc2)cc1Nc1nccc(-c2cccnc2)n1\\nThought: I need to check if this compound is novel\\nAction: Patent Search\\nAction Input: Cc1ccc(NC(=O)c2ccc(CN3CCNCC3)cc2)cc1Nc1nccc(-c2cccnc2)n1\\nObservation: Novel\\nThought: I need to purchase this compound\\nAction: Purchase\\nAction Input: Cc1ccc(NC(=O)c2ccc(CN3CCNCC3)cc2)cc1Nc1nccc(-c2cccnc2)n1\\nObservation: Purchased from ZINC\\nThought: I now know the ﬁnal answer\\nFinal Answer: The compound Cc1ccc(NC(=O)c2ccc(CN3CCNCC3)cc2)cc1Nc1nccc(-c2cccnc2)n1 is a novel com-\\npound with similar properties to Dasatinib and has been purchased from ZINC.\\n100')]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. BM + Hybrid Retriever"
      ],
      "metadata": {
        "id": "EycQ2t8RRF_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Assuming you have a Chroma database named 'chroma_db'\n",
        "chroma_db = chroma_db_openai\n",
        "\n",
        "# Define a function to create a BM25 index\n",
        "def create_bm25_index(tokenized_corpus):\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    return BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Define a function to perform BM25 search\n",
        "def bm25_search(bm25, query, k=5):\n",
        "    tokenized_query = query.split()\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    top_n = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    return [(i, scores[i]) for i in top_n]\n",
        "\n",
        "# Define a hybrid retriever class\n",
        "class HybridRetriever:\n",
        "    def __init__(self, chroma_db, bm25, k=5):\n",
        "        self.chroma_db = chroma_db\n",
        "        self.bm25 = bm25\n",
        "        self.k = k\n",
        "\n",
        "    def invoke(self, query):\n",
        "        # Perform vector search\n",
        "        vector_hits = self.chroma_db.similarity_search_with_score(query, k=10)\n",
        "\n",
        "        # Perform BM25 search\n",
        "        bm25_hits = bm25_search(self.bm25, query, k=10)\n",
        "\n",
        "        # Combine results\n",
        "        combined = []\n",
        "        max_vector = max([score for (_, score) in vector_hits]) if vector_hits else 1.0\n",
        "        max_bm25 = max([score for (_, score) in bm25_hits]) if bm25_hits else 1.0\n",
        "\n",
        "        for (doc, vscore) in vector_hits:\n",
        "            vec_norm = vscore / max_vector\n",
        "            bscore = next((bscore for idx, bscore in bm25_hits if idx == doc.id), 0.0) / max_bm25\n",
        "            combined_score = vec_norm + bscore\n",
        "            combined.append( (doc, combined_score) )\n",
        "\n",
        "        # Add any BM25-only docs not in vector hits\n",
        "        for idx, bscore in bm25_hits:\n",
        "            if idx not in [x[0].id for x in combined]:\n",
        "                bnorm = bscore / max_bm25\n",
        "                combined.append( (split_docs[idx], bnorm) )\n",
        "\n",
        "        # Sort by combined score desc\n",
        "        combined_sorted = sorted(combined, key=lambda x: x[1], reverse=True)\n",
        "        return [x[0] for x in combined_sorted[:self.k]]\n",
        "\n",
        "# Initialize the BM25 index\n",
        "tokenized_corpus = [doc.page_content.split() for doc in split_docs]\n",
        "bm25 = create_bm25_index(tokenized_corpus)\n",
        "\n",
        "# Create a hybrid retriever\n",
        "hybrid_retriever = HybridRetriever(chroma_db, bm25)\n",
        "\n",
        "# Perform a query\n",
        "query = \"What is GPT?\"\n",
        "top_docs = hybrid_retriever.invoke(query)\n",
        "top_docs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNpGEkqlN-YB",
        "outputId": "f2810340-c5a9-4741-93f5-4e843597e706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 43, 'page_label': '44', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='2 GPT-4 Observed Safety Challenges\\nGPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\\ncoding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\\nalso present new safety challenges, which we highlight in this section.\\nWe conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations\\nhelped us gain an understanding of GPT-4’s capabilities, limitations, and risks; prioritize our\\nmitigation eﬀorts; and iteratively test and build safer versions of the model. Some of the speciﬁc\\nrisks we explored are: 6\\n• Hallucinations\\n• Harmful content\\n• Harms of representation, allocation, and quality of service\\n• Disinformation and inﬂuence operations\\n• Proliferation of conventional and unconventional weapons\\n• Privacy\\n• Cybersecurity\\n• Potential for risky emergent behaviors\\n• Interactions with other systems\\n• Economic impacts\\n• Acceleration\\n• Overreliance'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 43, 'page_label': '44', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='2 GPT-4 Observed Safety Challenges\\nGPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\\ncoding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\\nalso present new safety challenges, which we highlight in this section.\\nWe conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations\\nhelped us gain an understanding of GPT-4’s capabilities, limitations, and risks; prioritize our\\nmitigation eﬀorts; and iteratively test and build safer versions of the model. Some of the speciﬁc\\nrisks we explored are: 6\\n• Hallucinations\\n• Harmful content\\n• Harms of representation, allocation, and quality of service\\n• Disinformation and inﬂuence operations\\n• Proliferation of conventional and unconventional weapons\\n• Privacy\\n• Cybersecurity\\n• Potential for risky emergent behaviors\\n• Interactions with other systems\\n• Economic impacts\\n• Acceleration\\n• Overreliance'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-11T01:02:02+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/GPT4.pdf', 'total_pages': 100, 'page': 35, 'page_label': '36'}, page_content='GPT-4 visual input example, Extreme Ironing:\\nUser What is unusual about this image?\\nSource: https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg\\nGPT-4 The unusual thing about this image is that a man is ironing clothes on an ironing\\nboard attached to the roof of a moving taxi.\\nTable 16. Example prompt demonstrating GPT-4’s visual input capability. The prompt requires image\\nunderstanding.\\n36'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 84, 'page_label': '85', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4 (launch) response\\nMy apologies, but I cannot provide you with assistance on illegal activities such as money laundering. If you\\nhave any other topic or question you’d like help with, feel free to ask.\\n85'),\n",
              " Document(metadata={'author': '', 'creationdate': '2024-03-11T01:02:02+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-03-11T01:02:02+00:00', 'page': 84, 'page_label': '85', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/GPT4.pdf', 'subject': '', 'title': '', 'total_pages': 100, 'trapped': '/False'}, page_content='GPT-4 (launch) response\\nMy apologies, but I cannot provide you with assistance on illegal activities such as money laundering. If you\\nhave any other topic or question you’d like help with, feel free to ask.\\n85')]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x6DCwR6dRZnf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}