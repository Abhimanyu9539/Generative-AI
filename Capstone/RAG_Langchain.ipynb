{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain --quiet\n",
        "!pip install langchain-openai --quiet\n",
        "!pip install langchain-community --quiet\n",
        "!pip install streamlit --quiet\n",
        "!pip install PyMuPDF --quiet\n",
        "!pip install chromadb --quiet\n",
        "!pip install pyngrok --quiet\n",
        "!pip install langchain-community --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynn5-SOqvvyF",
        "outputId": "e6090172-5ea0-4f4f-d502-0296fdbb324c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.1/571.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_API_KEY')\n",
        "os.environ['NGROK_AUTH_TOKEN'] = userdata.get('NGROK_AUTH_TOKEN')\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "-Y6689g-qBcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_core.callbacks.base import BaseCallbackHandler\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "from operator import itemgetter\n",
        "from langchain.retrievers import (\n",
        "    MultiQueryRetriever,\n",
        "    ContextualCompressionRetriever,\n",
        "    )\n",
        "\n",
        "from langchain.retrievers.document_compressors import (\n",
        "    LLMChainExtractor,\n",
        "    LLMChainFilter,\n",
        "    CrossEncoderReranker\n",
        "    )\n",
        "\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Customize initial app landing page\n",
        "st.set_page_config(page_title=\"File QA Chatbot\", page_icon=\"🤖\")\n",
        "st.title(\"Let's Chat With Your Files 🤖\")\n",
        "\n",
        "@st.cache_resource(ttl=\"1h\")\n",
        "def configure_retriever(uploaded_files):\n",
        "    # Read documents\n",
        "    docs = []\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    for file in uploaded_files:\n",
        "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "        with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(file.getvalue())\n",
        "        loader = PyMuPDFLoader(temp_filepath)\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    # Split into document chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "    doc_chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Create document embeddings and store in Vector DB\n",
        "    embeddings_model = OpenAIEmbeddings()\n",
        "    #vectordb = Chroma.from_documents(doc_chunks, embeddings_model)\n",
        "    #vectordb = Chroma.from_documents(doc_chunks, embeddings_model, persist_directory=None, in_memory=True)\n",
        "    persist_directory = \"./chroma_db\"\n",
        "    vectordb = Chroma.from_documents(doc_chunks, embeddings_model, persist_directory=persist_directory)\n",
        "\n",
        "    ##################################################################\n",
        "    # The following combination of Reranker, Filter and Contextual Compression\n",
        "    # Retriever takes much more time and almost gives similar output to that of\n",
        "    # normal similarity based retriever.\n",
        "    # Hence using a normal similarity based retriever only.\n",
        "\n",
        "    ##################################################################\n",
        "\n",
        "\n",
        "    # download an open-source reranker model - BAAI/bge-reranker-v2-m3\n",
        "    #reranker = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\n",
        "    #reranker_compressor = CrossEncoderReranker(model=reranker, top_n=3)\n",
        "\n",
        "    # Define retriever object\n",
        "    #chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
        "\n",
        "    #similarity_retriever_oai = vectordb.as_retriever(search_type=\"similarity\",\n",
        "     #                                         search_kwargs={\"k\": 5})\n",
        "    #_filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
        "    #compressor_retriever_oai = ContextualCompressionRetriever(\n",
        "    #base_compressor=_filter, base_retriever=similarity_retriever_oai)\n",
        "\n",
        "    # Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "    #final_retriever_oai = ContextualCompressionRetriever(\n",
        "    #base_compressor=reranker_compressor, base_retriever=compressor_retriever_oai)\n",
        "\n",
        "    retriever = vectordb.as_retriever()\n",
        "    return retriever\n",
        "\n",
        "# Callback to update UI as LLM streams responses\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container, initial_text=\"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "\n",
        "# Create UI element to accept PDF uploads\n",
        "uploaded_files = st.sidebar.file_uploader(\n",
        "    label=\"Upload PDF files\", type=[\"pdf\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "if not uploaded_files:\n",
        "    st.info(\"Please upload PDF documents to continue.\")\n",
        "    st.stop()\n",
        "\n",
        "# Create retriever object based on uploaded PDFs\n",
        "retriever = configure_retriever(uploaded_files)\n",
        "\n",
        "# Load a connection to ChatGPT LLM\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.1, streaming=True)\n",
        "\n",
        "# Update the prompt template to include chat history context\n",
        "qa_template = \"\"\"\n",
        "Use the following conversation history:\n",
        "{chat_history}\n",
        "\n",
        "Use only the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know,\n",
        "don't try to make up an answer. Keep the answer as concise as possible.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
        "\n",
        "# This function formats retrieved documents before sending to LLM\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "\n",
        "# Create a QA RAG System Chain that now maps the user question, chat history, and retrieved context.\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\")  # Retrieve context based on question\n",
        "            | retriever\n",
        "            | format_docs,\n",
        "        \"question\": itemgetter(\"question\"),  # User question\n",
        "        \"chat_history\": itemgetter(\"chat_history\")  # Conversation history\n",
        "    }\n",
        "    | qa_prompt  # Pass to prompt template which now expects chat_history, context, and question\n",
        "    | chatgpt   # Send prompt to LLM for response\n",
        ")\n",
        "\n",
        "\n",
        "# Store conversation history in Streamlit session state\n",
        "streamlit_msg_history = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
        "\n",
        "# Shows the first message when the app starts\n",
        "if len(streamlit_msg_history.messages) == 0:\n",
        "    streamlit_msg_history.add_ai_message(\"What would you like to ask to your documents?\")\n",
        "\n",
        "# Render current messages from StreamlitChatMessageHistory\n",
        "for msg in streamlit_msg_history.messages:\n",
        "    st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "# Callback handler to display top 3 document sources used by the LLM\n",
        "class PostMessageHandler(BaseCallbackHandler):\n",
        "    def __init__(self, msg: st.write):\n",
        "        BaseCallbackHandler.__init__(self)\n",
        "        self.msg = msg\n",
        "        self.sources = []\n",
        "\n",
        "    def on_retriever_end(self, documents, *, run_id, parent_run_id, **kwargs):\n",
        "        source_ids = []\n",
        "        for d in documents:  # Retrieved documents from retriever based on user query\n",
        "            metadata = {\n",
        "                \"source\": d.metadata[\"source\"],\n",
        "                \"page\": d.metadata[\"page\"],\n",
        "                \"content\": d.page_content[:200]\n",
        "            }\n",
        "            idx = (metadata[\"source\"], metadata[\"page\"])\n",
        "            if idx not in source_ids:  # Store unique source documents\n",
        "                source_ids.append(idx)\n",
        "                self.sources.append(metadata)\n",
        "\n",
        "    def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
        "        if len(self.sources):\n",
        "            st.markdown(\"__Sources:__\\n\")\n",
        "            st.dataframe(data=pd.DataFrame(self.sources[:3]), width=1000)  # Top 3 sources\n",
        "\n",
        "# Helper function to compile chat history from message objects\n",
        "def compile_chat_history(messages):\n",
        "    history = \"\"\n",
        "    for msg in messages:\n",
        "        # Capitalize role for clarity\n",
        "        role = msg.type.capitalize()\n",
        "        history += f\"{role}: {msg.content}\\n\"\n",
        "    return history\n",
        "\n",
        "# If user inputs a new prompt, display it and show the response\n",
        "if user_prompt := st.chat_input():\n",
        "    # Append human message to chat history and display it\n",
        "    st.chat_message(\"human\").write(user_prompt)\n",
        "    streamlit_msg_history.add_user_message(user_prompt)\n",
        "\n",
        "    # Compile chat history to pass to the chain\n",
        "    chat_history_text = compile_chat_history(streamlit_msg_history.messages)\n",
        "\n",
        "    # Use an AI message container for the response\n",
        "    with st.chat_message(\"ai\"):\n",
        "        stream_handler = StreamHandler(st.empty())\n",
        "        sources_container = st.write(\"\")\n",
        "        pm_handler = PostMessageHandler(sources_container)\n",
        "        config = {\"callbacks\": [stream_handler, pm_handler]}\n",
        "\n",
        "        # Pass both question and compiled chat history to the chain\n",
        "        response = qa_rag_chain.invoke({\"question\": user_prompt, \"chat_history\": chat_history_text}, config)\n",
        "        # Add AI's response to the message history\n",
        "        streamlit_msg_history.add_ai_message(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToOCn3B3oSpJ",
        "outputId": "94315f06-2d08-42c3-a2bd-4fbd1ebce1c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "soaVfBYCo-6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import yaml\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken\n",
        "# Get your authtoken from `ngrok_credentials.yml` file\n",
        "# with open('ngrok_credentials.yml', 'r') as file:\n",
        "#     NGROK_AUTH_TOKEN = yaml.safe_load(file)\n"
      ],
      "metadata": {
        "id": "qEz8PjJ5pRwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(os.environ['NGROK_AUTH_TOKEN'] )\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqvTezfjtM1v",
        "outputId": "bd0e1ee6-0776-43aa-beba-817240fa2859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://eb1c-35-245-21-126.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}