{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index --quiet\n",
        "!pip install llama-index-embeddings-huggingface --quiet\n",
        "!pip install llama-index-llms-huggingface --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYszXeAoak3y",
        "outputId": "ec6ebb9e-b943-4ec1-fafa-9523125884df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-huggingface-api --quiet\n",
        "!pip install llama-index-embeddings-openai --quiet"
      ],
      "metadata": {
        "id": "jNwcRddUfLGv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-oX-E10lZWGP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Model\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding"
      ],
      "metadata": {
        "id": "15oRQtE0eXCU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define embedding models\n",
        "embedding_models = {\n",
        "    \"openai\": OpenAIEmbedding(model=\"text-embedding-3-large\"),\n",
        "    \"bge\": HuggingFaceInferenceAPIEmbedding(model_name=\"BAAI/bge-large-en\"),\n",
        "    \"jina\": HuggingFaceInferenceAPIEmbedding(model_name=\"jinaai/jina-embeddings-v2-base-en\")\n",
        "}"
      ],
      "metadata": {
        "id": "8XF22kHzadwN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LLMs\n",
        "llms = {\n",
        "    \"gpt-4\": OpenAI(model=\"gpt-4o-mini\"),\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "FypuYsugb3WJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(input_files=['/content/data/Final Policy document_LICs New Jeevan Shanti_V05_logo.pdf']).load_data()"
      ],
      "metadata": {
        "id": "1SyZwRnHcADh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over embedding models for indexing\n",
        "vector_indexes = {}\n",
        "for name, embed_model in embedding_models.items():\n",
        "    index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
        "    vector_indexes[name] = index"
      ],
      "metadata": {
        "id": "H37LRm0AcHRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate retrieval and synthesis\n",
        "for embed_name, index in vector_indexes.items():\n",
        "    for llm_name, llm in llms.items():\n",
        "        retriever = index.as_retriever()\n",
        "        evaluator = FaithfulnessEvaluator()\n",
        "        query = \"Summarize the document contents.\"\n",
        "        retrieved_docs = retriever.retrieve(query)\n",
        "        response = llm.complete(query)\n",
        "        score = evaluator.evaluate(response, retrieved_docs)\n",
        "        print(f\"Embedding: {embed_name}, LLM: {llm_name}, Faithfulness Score: {score}\")"
      ],
      "metadata": {
        "id": "3FX24N0dc5Dz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}