{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Legacy Chains in LangChain"
      ],
      "metadata": {
        "id": "TkZ-tYIsqsAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.2.0\n",
        "!pip install langchain-openai==0.1.7\n",
        "!pip install langchain-community==0.2.0"
      ],
      "metadata": {
        "id": "2evPp14fy258"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "id": "cv3JzCEx_PAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "1T0s0um5Svfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ],
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Connection to LLM\n",
        "\n",
        "Here we create a connection to ChatGPT to use later in our chains"
      ],
      "metadata": {
        "id": "2oeckxFBcc0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)"
      ],
      "metadata": {
        "id": "vHa9LMOfcOCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with LangChain Chains\n",
        "\n",
        "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. Also running on multiple data points can be done easily with chains.\n",
        "\n",
        "Chain's are the legacy interface for \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains.\n",
        "\n",
        "Here we use LangChain's Legacy Chains which are going to get ported over to LCEL chains over time"
      ],
      "metadata": {
        "id": "FD3EKYm8M8gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with LLMChain\n",
        "\n",
        "The most common type of chaining in any LLM application is combining a prompt template with an LLM and optionally an output parser.\n",
        "\n",
        "An `LLMChain` is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.\n",
        "\n",
        "An `LLMChain` consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\n",
        "\n",
        "__Note:__ `LLMChain` has been deprecated by LangChain in favor of using an LCEL variant, we will be looking at that in a subsequent demo on LCEL chains"
      ],
      "metadata": {
        "id": "ng-9uxLActTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "    f\"\"\"\n",
        "    Purchased this adorable koala plush toy for my nephew's birthday,\n",
        "    and he's absolutely smitten with it, carrying it around everywhere he goes.\n",
        "    The plush is incredibly soft, and the koala's face has an endearing expression.\n",
        "    However, I did find it a tad on the smaller side given its price point.\n",
        "    I believe there may be larger alternatives available at a similar price.\n",
        "    To my delight, it arrived a day earlier than anticipated,\n",
        "    allowing me to enjoy it briefly before gifting it to him.\n",
        "    \"\"\",\n",
        "    f\"\"\"\n",
        "    Required a stylish lamp for my office space, and this particular one\n",
        "    came with added storage at a reasonable price.\n",
        "    The delivery was surprisingly quick, arriving within just two days.\n",
        "    However, the pull string for the lamp suffered damage during transit.\n",
        "    To my relief, the company promptly dispatched a replacement,\n",
        "    which arrived within a few days. Assembly was a breeze.\n",
        "    Then, I encountered an issue with a missing component,\n",
        "    but their support team responded swiftly and provided the missing part.\n",
        "    It appears to be a commendable company that genuinely values its\n",
        "    customers and the quality of its products.\n",
        "    \"\"\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "LPSWayj_cP-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = \"\"\"\n",
        "            Act as a product review analyst.\n",
        "            Your task is to generate a short summary of a product\n",
        "            review from an ecommerce site.\n",
        "\n",
        "            Generate a summary of the review (max 2 lines)\n",
        "            Also show both the positives and negatives from the review (max 2 bullets)\n",
        "\n",
        "            ```{review}```\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
        "llm_chain = LLMChain(llm=chatgpt, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "90dSQ0yciLQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[0]"
      ],
      "metadata": {
        "id": "ZNomLQEPSq7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm_chain.invoke({'review': reviews[0]})"
      ],
      "metadata": {
        "id": "0SjsgsUdiibW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "lL0EKbgXivEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "id": "2WsezaGqi00W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_reviews = [{'review': review}\n",
        "                        for review in reviews]\n",
        "\n",
        "results = llm_chain.map().invoke(formatted_reviews)\n",
        "len(results)"
      ],
      "metadata": {
        "id": "KNLieaWvi2XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for result in results:\n",
        "    print(result['text'])\n",
        "    print()"
      ],
      "metadata": {
        "id": "AtaDS_IXjhRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining Multiple Tasks with SequentialChain\n",
        "\n",
        "The next step after calling a language model is to make a series of calls to a language model. This is particularly useful when you want to take the output from one call and use it as the input to another.\n",
        "\n",
        "Sequential chains allow you to connect multiple chains and compose them into pipelines that execute some specific scenario."
      ],
      "metadata": {
        "id": "qNbZQZEqSFFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here given a few IT Support issues, for each customer issue:\n",
        "- We want to detect the customer message language\n",
        "- We want to translate the customer message from the source language to English\n",
        "- We want the AI to generate a suitable response to the problem in English\n",
        "- We want to translate this response to the customer language"
      ],
      "metadata": {
        "id": "9NmswXRAT76C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "it_support_queue = [\n",
        "    \"I can't access my email. It keeps showing an error message. Please help.\",\n",
        "    \"Tengo problemas con la VPN. No puedo conectarme a la red de la empresa. ¿Pueden ayudarme, por favor?\",\n",
        "    \"Mon imprimante ne répond pas et n'imprime plus. J'ai besoin d'aide pour la réparer.\",\n",
        "    \"我无法访问公司的网站。每次都显示错误信息。请帮忙解决。\"\n",
        "]\n",
        "\n",
        "it_support_queue"
      ],
      "metadata": {
        "id": "hXWJuNibZrmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can use `SequentialChain` and chain multiple `LLMChains` where each chain solves one of the tasks using a specific prompt as follows:"
      ],
      "metadata": {
        "id": "8Vo1keR8U8MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain 1: Detect customer message language\n",
        "prompt1 = \"\"\"\n",
        "  Act as a customer support agent.\n",
        "  For the customer support message delimited below by triple backticks,\n",
        "  Output the language of the message in one word only, e.g. Spanish\n",
        "\n",
        "  Customer Message:\n",
        "  ```{orig_msg}```\n",
        "\"\"\"\n",
        "prompt_template1 = ChatPromptTemplate.from_template(prompt1)\n",
        "chain1 = LLMChain(llm=chatgpt, prompt=prompt_template1, output_key=\"orig_lang\")\n",
        "\n",
        "# Chain 2: Translate Customer Message to English\n",
        "prompt2 = \"\"\"\n",
        "  Act as a customer support agent.\n",
        "  For the customer message and customer message language delimited below by triple backticks,\n",
        "  Translate the customer message from the customer message language to English\n",
        "  if customer message language is not in English,\n",
        "  else return back the original customer message.\n",
        "\n",
        "  Customer Message:\n",
        "  ```{orig_msg}```\n",
        "  Customer Message Language:\n",
        "  ```{orig_lang}```\n",
        "\"\"\"\n",
        "prompt_template2 = ChatPromptTemplate.from_template(prompt2)\n",
        "chain2 = LLMChain(llm=chatgpt, prompt=prompt_template2, output_key=\"trans_msg\")\n",
        "\n",
        "# Chain 3: Generate a resolution response in English\n",
        "prompt3 = \"\"\"\n",
        "  Act as a customer support agent.\n",
        "  For the customer support message delimited below by triple backticks,\n",
        "  Generate an appropriate resolution response in English.\n",
        "\n",
        "  Customer Message:\n",
        "  ```{trans_msg}```\n",
        "\"\"\"\n",
        "prompt_template3 = ChatPromptTemplate.from_template(prompt3)\n",
        "chain3 = LLMChain(llm=chatgpt, prompt=prompt_template3, output_key=\"trans_response\")\n",
        "\n",
        "# Chain 4: Translate resolution response from English to Customer's language\n",
        "prompt4 = \"\"\"\n",
        "  Act as a customer support agent.\n",
        "  For the customer resolution response and target language delimited below by triple backticks,\n",
        "  Translate the customer resolution response message from English to the target language\n",
        "  if target language is not in English,\n",
        "  else return back the original customer resolution response.\n",
        "\n",
        "  Customer Resolution Response:\n",
        "  ```{trans_response}```\n",
        "  Target Language:\n",
        "  ```{orig_lang}```\n",
        "\"\"\"\n",
        "prompt_template4 = ChatPromptTemplate.from_template(prompt4)\n",
        "chain4 = LLMChain(llm=chatgpt, prompt=prompt_template4, output_key=\"response\")"
      ],
      "metadata": {
        "id": "xeMwJkMnxCRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# combine all chains sequentially\n",
        "seq_chain = SequentialChain(\n",
        "    chains=[chain1, chain2, chain3, chain4],\n",
        "    input_variables=[\"orig_msg\"], # input data\n",
        "    output_variables=[\"orig_msg\", \"orig_lang\",\"trans_msg\", \"response\", \"trans_response\"], # response data\n",
        "    verbose=True # set to False to turn of debugging messages\n",
        ")"
      ],
      "metadata": {
        "id": "wO6OZX2p1TfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "support_msgs_formatted = [{'orig_msg': msg} for msg in it_support_queue]\n",
        "support_msgs_formatted"
      ],
      "metadata": {
        "id": "dxEMxKN81pfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the chain on all the support messages\n",
        "results = seq_chain.map().invoke(support_msgs_formatted)"
      ],
      "metadata": {
        "id": "njapn8-y1iJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[2]"
      ],
      "metadata": {
        "id": "rLG_FG4Bjmdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df"
      ],
      "metadata": {
        "id": "fSWyZ8nKoUkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other popular Legacy Chains include:\n",
        "\n",
        "- `ConversationChain`\n",
        "- `ConversationalRetrievalChain`\n",
        "- `RetrievalQA`\n",
        "\n",
        "However given LangChain's focus on LCEL Chains, we will be focusing on implementing all of these using LCEL Chains soon."
      ],
      "metadata": {
        "id": "hLjK3s6do0Ee"
      }
    }
  ]
}