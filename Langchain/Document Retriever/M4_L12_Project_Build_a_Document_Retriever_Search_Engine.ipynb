{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkZ-tYIsqsAP"
   },
   "source": [
    "# Project: Build a Document Retriever Search Engine on Wikipedia Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1KvMtf54l0d"
   },
   "source": [
    "## Install OpenAI, and LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2evPp14fy258"
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.2.0\n",
    "!pip install langchain-openai==0.1.7\n",
    "!pip install langchain-community==0.2.0\n",
    "!pip install sentence-transformers==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlfidBdQZRGj"
   },
   "source": [
    "## Install Chroma Vector DB and LangChain wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZKQDgQURhmF"
   },
   "outputs": [],
   "source": [
    "!pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9c37cLnSrbg"
   },
   "source": [
    "## Enter Open AI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv3JzCEx_PAd"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T0s0um5Svfa"
   },
   "source": [
    "## Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1YSuHNF_lbh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8nHAP7XOGOj"
   },
   "source": [
    "### Open AI Embedding Models\n",
    "\n",
    "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzrIVI2NAHC1"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IO3_BzTbTBcJ"
   },
   "source": [
    "## Vector Databases\n",
    "\n",
    "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector database takes care of storing embedded data and performing vector search for you.\n",
    "\n",
    "### Chroma Vector DB\n",
    "\n",
    "[Chroma](https://docs.trychroma.com/getting-started) is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA_-hzHbFeSP"
   },
   "source": [
    "### Get the wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZFMYH-yFhWn"
   },
   "outputs": [],
   "source": [
    "# if you can't download using the following code\n",
    "# go to https://drive.google.com/file/d/1oWBnoxBZ1Mpeond8XDUSO6J9oAjcRDyW download it\n",
    "# manually upload it on colab\n",
    "!gdown 1oWBnoxBZ1Mpeond8XDUSO6J9oAjcRDyW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwLEBC4nF9ly"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "docs = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        #Add all paragraphs\n",
    "        #passages.extend(data['paragraphs'])\n",
    "\n",
    "        #Only add the first paragraph\n",
    "        docs.append({\n",
    "                        'metadata': {\n",
    "                                        'title': data.get('title'),\n",
    "                                        'article_id': data.get('id')\n",
    "                        },\n",
    "                        'data': ' '.join(data.get('paragraphs')[0:3]) # restrict data to first 3 paragraphs to run later modules faster\n",
    "        })\n",
    "\n",
    "print(\"Passages:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oABVR8m-6ZiY"
   },
   "outputs": [],
   "source": [
    "# We subset our data so we only use a subset of wikipedia documents to run things faster\n",
    "docs = [doc for doc in docs for x in ['linguistics', 'india', 'cheetah']\n",
    "              if x in doc['data'].lower().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4E1zYFSG7J-"
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSbhERAyGw0v"
   },
   "outputs": [],
   "source": [
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPjmN39-Nv1Y"
   },
   "source": [
    "### Create LangChain Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9KThjCwN0Ez"
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=doc['data'],\n",
    "                 metadata=doc['metadata']) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFMT5Y0xN_PV"
   },
   "outputs": [],
   "source": [
    "docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccGsE7JYOilo"
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaSljqmlOFwm"
   },
   "source": [
    "### Split larger documents into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-f9rqPnROE82"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqqp0RdkOsrg"
   },
   "outputs": [],
   "source": [
    "chunked_docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-1rG2xnOqcn"
   },
   "outputs": [],
   "source": [
    "len(chunked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PnV9lAXZw9a"
   },
   "source": [
    "### Create a Vector DB and persist on disk\n",
    "\n",
    "Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRYfcrsHUxyZ"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
    "chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
    "                                  collection_name='wikipedia_db',\n",
    "                                  embedding=openai_embed_model,\n",
    "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
    "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                                  persist_directory=\"./wikipedia_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ju_zBIj1Zsb"
   },
   "source": [
    "### Load Vector DB from disk\n",
    "\n",
    "This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNvj0dDH1WDg"
   },
   "outputs": [],
   "source": [
    "# load from disk\n",
    "chroma_db = Chroma(persist_directory=\"./wikipedia_db\",\n",
    "                   collection_name='wikipedia_db',\n",
    "                   embedding_function=openai_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFC3uPqYop0a"
   },
   "outputs": [],
   "source": [
    "chroma_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bprZC4S6TLfj"
   },
   "source": [
    "## Experiment with Vector Database Retrievers\n",
    "\n",
    "Here we will explore the following retrieval strategies on our Vector Database:\n",
    "\n",
    "- Similarity or Ranking based Retrieval\n",
    "- Multi Query Retrieval\n",
    "- Contextual Compression Retrieval\n",
    "- Chained Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "269fFuebBr5P"
   },
   "source": [
    "### Similarity or Ranking based Retrieval\n",
    "\n",
    "We use cosine similarity here and retrieve the top 3 similar documents based on the user input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST7PDWG2b0ln"
   },
   "outputs": [],
   "source": [
    "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4czRBjWLe8JH"
   },
   "outputs": [],
   "source": [
    "query = \"what is the capital of India?\"\n",
    "top3_docs = similarity_retriever.invoke(query)\n",
    "top3_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYGmZdV--qz_"
   },
   "outputs": [],
   "source": [
    "query = \"what is the old capital of India?\"\n",
    "top3_docs = similarity_retriever.invoke(query)\n",
    "top3_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJmJRjlV_kvS"
   },
   "outputs": [],
   "source": [
    "query = \"what is the fastest animal?\"\n",
    "top3_docs = similarity_retriever.invoke(query)\n",
    "top3_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lfhHB7gQP3Q"
   },
   "outputs": [],
   "source": [
    "query = \"what is the slowest animal?\"\n",
    "top3_docs = similarity_retriever.invoke(query)\n",
    "top3_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cT3CFWhVRo2f"
   },
   "outputs": [],
   "source": [
    "query = \"what is the study of language?\"\n",
    "top3_docs = similarity_retriever.invoke(query)\n",
    "top3_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HKdIe2_wIBR"
   },
   "source": [
    "### Multi Query Retrieval\n",
    "\n",
    "Retrieval may produce different results with subtle changes in query wording, or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\n",
    "\n",
    "The [`MultiQueryRetriever`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html) automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmfVRrRujxrA"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3gU4HPvfjs7"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 3})\n",
    "\n",
    "mq_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=similarity_retriever, llm=chatgpt\n",
    ")\n",
    "\n",
    "logging.basicConfig()\n",
    "# so we can see what queries are generated by the LLM\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZJBIsl8xC_N"
   },
   "outputs": [],
   "source": [
    "query = \"what is the capital of India?\"\n",
    "docs = mq_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVBeusuSyVo3"
   },
   "outputs": [],
   "source": [
    "query = \"what is the old capital of India?\"\n",
    "docs = mq_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5yTCexbSUTU"
   },
   "outputs": [],
   "source": [
    "query = \"what is the study of language?\"\n",
    "docs = mq_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKkMcQpf3lBs"
   },
   "source": [
    "### Contextual Compression Retrieval\n",
    "\n",
    "The information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned.\n",
    "\n",
    "This compression can happen in the form of:\n",
    "\n",
    "- Remove parts of the content of retrieved documents which are not relevant to the query. This is done by extracting only relevant parts of the document to the given query\n",
    "\n",
    "- Filter out documents which are not relevant to the given query but do not remove content from the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNMpbsi64QcM"
   },
   "source": [
    "Here we wrap our multi-query retriever with a `ContextualCompressionRetriever`. Then we'll add an `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b14n_2AHykTf"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "\n",
    "# extracts from each document only the content that is relevant to the query\n",
    "compressor = LLMChainExtractor.from_llm(llm=chatgpt)\n",
    "\n",
    "# retrieves the documents similar to query and then applies the compressor\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=mq_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8yvjFWCTew6"
   },
   "outputs": [],
   "source": [
    "query = \"what is the financial capital of India?\"\n",
    "docs = compression_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW64q3FF0M-b"
   },
   "outputs": [],
   "source": [
    "query = \"what is the old capital of India?\"\n",
    "docs = compression_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98EVsL_dTd6k"
   },
   "outputs": [],
   "source": [
    "query = \"what is the fastest animal?\"\n",
    "docs = compression_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onRoGL4TTk9C"
   },
   "outputs": [],
   "source": [
    "query = \"what is the slowest animal?\"\n",
    "docs = compression_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QR_QNS8Scvu"
   },
   "outputs": [],
   "source": [
    "query = \"what is the study of language?\"\n",
    "docs = compression_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URTONl5c4aLe"
   },
   "source": [
    "The `LLMChainFilter` is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fs-INms_0VTX"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "\n",
    "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
    "_filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
    "\n",
    "# retrieves the documents similar to query and then applies the filter\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=mq_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgKB3_lV17Hh"
   },
   "outputs": [],
   "source": [
    "query = \"what is the financial capital of India?\"\n",
    "docs = compression_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyhPhAld1-H3"
   },
   "outputs": [],
   "source": [
    "query = \"what is the old capital of India?\"\n",
    "docs = compression_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ap7ddchJT0BH"
   },
   "outputs": [],
   "source": [
    "query = \"what is the fastest animal?\"\n",
    "docs = compression_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-0x-01-Uf2G"
   },
   "outputs": [],
   "source": [
    "query = \"what is the slowest animal?\"\n",
    "docs = compression_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YK8BBMBAU9o"
   },
   "source": [
    "### Chained Retrieval Pipeline\n",
    "\n",
    "This strategy uses a chain of multiple retrievers sequentially to get to the most relevant documents. The following is the flow\n",
    "\n",
    "Similarity Retrieval → Compression Filter → Reranker Model Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgZ4eMFc2GgI"
   },
   "outputs": [],
   "source": [
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "\n",
    "# Retriever 1 - simple cosine distance based retriever\n",
    "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 5})\n",
    "\n",
    "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
    "_filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
    "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
    "compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=similarity_retriever\n",
    ")\n",
    "\n",
    "# download an open-source reranker model - BAAI/bge-reranker-v2-m3\n",
    "reranker = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\n",
    "reranker_compressor = CrossEncoderReranker(model=reranker, top_n=3)\n",
    "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
    "final_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker_compressor, base_retriever=compressor_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dVz4JGVBQTO"
   },
   "outputs": [],
   "source": [
    "query = \"what is the financial capital of India?\"\n",
    "docs = final_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFtoRwndBZuc"
   },
   "outputs": [],
   "source": [
    "query = \"what is the old capital of India?\"\n",
    "docs = final_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nKGhybhZVQF"
   },
   "outputs": [],
   "source": [
    "query = \"what is the fastest animal?\"\n",
    "docs = final_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCV6bZN8Zkn5"
   },
   "outputs": [],
   "source": [
    "query = \"what is the slowest animal?\"\n",
    "docs = final_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLuSVax9ZnGc"
   },
   "outputs": [],
   "source": [
    "query = \"what is the study of language?\"\n",
    "docs = final_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONMSpCItZovi"
   },
   "outputs": [],
   "source": [
    "query = \"what is chess?\"\n",
    "docs = final_retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKnF_Zbsbx8a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
