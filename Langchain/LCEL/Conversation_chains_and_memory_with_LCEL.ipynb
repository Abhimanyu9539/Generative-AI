{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Q9Y23CCXIv",
        "outputId": "476b06b4-41c8-490e-f0d4-b3f75bdf4dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/973.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.2.0 --quiet\n",
        "!pip install langchain-openai==0.1.7 --quiet\n",
        "!pip install langchain-community==0.2.0 --quiet\n",
        "!pip install langchain-chroma==0.1.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_API_KEY')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "HTBjsHo1Cn8w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)"
      ],
      "metadata": {
        "id": "d0eM47tCCyUn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with LangChain Chains\n",
        "\n",
        "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. Also running on multiple data points can be done easily with chains.\n",
        "\n",
        "Chain's are the legacy interface for \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains.\n",
        "\n",
        "Here we will be using LCEL chains exclusively"
      ],
      "metadata": {
        "id": "sC7d5hIyDCpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Problem with Simple LLM Chains\n",
        "\n",
        "Simple LLM Chains cannot keep a track of past conversation history"
      ],
      "metadata": {
        "id": "i0aupmqQDSa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "prompt_txt = \"\"\"{query}\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_txt)\n",
        "\n",
        "llm_chain = (prompt\n",
        "             |\n",
        "             chatgpt\n",
        "             )"
      ],
      "metadata": {
        "id": "xZGZ-1DdDVkF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_chain.invoke({'query': 'What are the first four colors of a rainbow?'})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aly49GiTDtUX",
        "outputId": "721f35bd-b6d8-4960-e166-2f69a5cf3fff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first four colors of a rainbow are red, orange, yellow, and green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_chain.invoke({'query': 'and the other 3?'})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYEyzp8vDxDa",
        "outputId": "753a68d8-1748-4ada-f0dd-0a56bed8b433"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, I'm not sure what you are referring to. Can you please provide more context or clarify your question?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with LCEL\n",
        "\n",
        "LangChain Expression Language (LCEL) connects prompts, models, parsers and retrieval components using a `|` pipe operator.\n",
        "\n",
        "A conversation chain basically consists of user prompts, historical conversation memory and the LLM. The LLM uses the history memory to give more contextual answers to every new prompt or user query.\n",
        "\n",
        "Memory is very important for having a true conversation with LLMs. LangChain allows us to manage conversation memory using various constructs. The main ones we will cover include:\n",
        "\n",
        "- ConversationBufferMemory\n",
        "- ConversationBufferWindowMemory\n",
        "- ConversationSummaryMemory\n",
        "- VectorStoreRetrieverMemory\n",
        "- ChatMessageHistory\n",
        "- SQLChatMessageHistory"
      ],
      "metadata": {
        "id": "VvpCox-tD07X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with ConversationBufferMemory\n",
        "\n",
        "This is the simplest version of in-memory storage of historical conversation messages. It is basically a buffer for storing conversation memory.\n",
        "\n",
        "Remember if you have a really long conversation, you might exceed the max token limit of the context window allowed for the LLM."
      ],
      "metadata": {
        "id": "dSDiIfTEEcEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ],
      "metadata": {
        "id": "MirMMXsHEf5N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get historical conversation messages from the memory\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60--JuLcGyVx",
        "outputId": "7b2a7de6-622f-4c42-bebb-6e92bde1eff7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets create a function now which returns the list of messages from memory\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "get_memory_messages('What are the first four colors of a rainbow?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEz5JB4YG0gy",
        "outputId": "55ef87cb-56ed-41a1-8479-d4d4491012a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing out the function with a runnable lambda which will go into our chain\n",
        "# this returns the history but we also need to send our current query to the prompt\n",
        "RunnableLambda(get_memory_messages).invoke({'query': 'What are the first four colors of a rainbow?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "870gOaOpG-QE",
        "outputId": "cc22e72a-d17e-4c02-d8cf-3c4cc81b3d74"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we use a runnable passthrough to pass our current query untouched\n",
        "# along with the history messages to the next step in the chain\n",
        "RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ).invoke({'query': 'What are the first four colors of a rainbow?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8LGrAefHAK-",
        "outputId": "6b07d073-535d-4b80-dead-91b0eaaeacda"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What are the first four colors of a rainbow?', 'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "X54ssoU-HDu1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What are the first four colors of a rainbow?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-yJg4xPHHN4",
        "outputId": "4b9008ae-263f-4fdf-eacd-04385ca1b486"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 30, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3ccac487-058f-4870-b2a9-a1ac112298bf-0')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peiuv6twHIoP",
        "outputId": "3cf181cb-2fd1-4d32-c954-af1bf09c06dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first four colors of a rainbow are red, orange, yellow, and green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txIO7e6MHOH9",
        "outputId": "600f3be6-c71f-4761-899d-576bad63b674"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(query, {\"output\": response.content})"
      ],
      "metadata": {
        "id": "a9-bnU3gHPmv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ct6kldyHRAz",
        "outputId": "e73d9809-0aef-4981-ed69-432eb0aa02a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What are the first four colors of a rainbow?'),\n",
              "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'and the other 3?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvlagz3MHSYX",
        "outputId": "f163434a-f599-4c4d-8df1-a313c573430a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The other three colors of a rainbow are blue, indigo, and violet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qeds6pNgHU3v",
        "outputId": "4f945d62-a0fc-4bbc-a7c6-bac3203b7312"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What are the first four colors of a rainbow?'),\n",
              "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.'),\n",
              "  HumanMessage(content='and the other 3?'),\n",
              "  AIMessage(content='The other three colors of a rainbow are blue, indigo, and violet.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Explain AI in 2 bullet points'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v25vUOtfHWyS",
        "outputId": "a3528920-e96b-4374-c06f-faabd2ac33e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\n",
            "- AI technologies include machine learning, natural language processing, computer vision, and robotics, and are used in various applications like virtual assistants, autonomous vehicles, and medical diagnosis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Now do the same for Deep Learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcX_FwhIHYkh",
        "outputId": "c862aeed-90bc-4b5d-e53b-230a0e5619e9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems.\n",
            "- It is particularly effective for tasks such as image and speech recognition, natural language processing, and autonomous driving, by learning representations of data through multiple layers of abstraction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQv3CgwPHaYO",
        "outputId": "64de4818-5546-4cf9-b3df-842231a31fb1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What are the first four colors of a rainbow?'),\n",
              "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.'),\n",
              "  HumanMessage(content='and the other 3?'),\n",
              "  AIMessage(content='The other three colors of a rainbow are blue, indigo, and violet.'),\n",
              "  HumanMessage(content='Explain AI in 2 bullet points'),\n",
              "  AIMessage(content='- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\\n- AI technologies include machine learning, natural language processing, computer vision, and robotics, and are used in various applications like virtual assistants, autonomous vehicles, and medical diagnosis.'),\n",
              "  HumanMessage(content='Now do the same for Deep Learning'),\n",
              "  AIMessage(content='- Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems.\\n- It is particularly effective for tasks such as image and speech recognition, natural language processing, and autonomous driving, by learning representations of data through multiple layers of abstraction.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What have we discussed so far?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi16XW8YHcY6",
        "outputId": "10d059b0-8847-44df-ae41-df44648422cf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have discussed the colors of a rainbow, artificial intelligence (AI), and deep learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with ConversationBufferWindowMemory\n",
        "\n",
        "If you have a really long conversation, you might exceed the max token limit of the context window allowed for the LLM when using `ConversationBufferMemory` so `ConversationBufferWindowMemory` helps in just storing the last K conversations (one conversation piece is one user message and the corresponding AI message from the LLM) and thus helps you manage token limits and costs"
      ],
      "metadata": {
        "id": "ieOMs1TSHeOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# stores last 2 sets of human-AI conversations\n",
        "memory = ConversationBufferWindowMemory(return_messages=True, k=2)\n",
        "\n",
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "zJWzWOqSMflz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What are the first four colors of a rainbow?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edukYgIsMkZA",
        "outputId": "3cc4a28f-4044-4d7d-e5a1-66d51f80d244"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first four colors of a rainbow are red, orange, yellow, and green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'and the other 3?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTl6jlICMld2",
        "outputId": "290bd472-e159-4d34-b958-452b97461f1b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The other three colors of a rainbow are blue, indigo, and violet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Explain AI in 2 bullet points'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRPHgxy8Mm6s",
        "outputId": "1ceefd7a-b60d-449d-982d-02965c82412c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\n",
            "- AI technologies include machine learning, natural language processing, computer vision, and robotics, among others.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Now do the same for Deep Learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3khlHPEMoMJ",
        "outputId": "cef4ec55-f318-4160-e0c3-8c3a5f00819f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems.\n",
            "- It is particularly effective for tasks such as image and speech recognition, natural language processing, and autonomous driving.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLlZCQ7YMo8_",
        "outputId": "e01582fc-da17-4f3e-972a-a1d3664aa19d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Explain AI in 2 bullet points'),\n",
              "  AIMessage(content='- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\\n- AI technologies include machine learning, natural language processing, computer vision, and robotics, among others.'),\n",
              "  HumanMessage(content='Now do the same for Deep Learning'),\n",
              "  AIMessage(content='- Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems.\\n- It is particularly effective for tasks such as image and speech recognition, natural language processing, and autonomous driving.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What have we discussed so far?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb_UhbK3Mp1c",
        "outputId": "1586c701-1a5f-4f97-d0a3-cd66a056467b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have discussed AI and deep learning, including their definitions and key characteristics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with ConversationSummaryMemory\n",
        "\n",
        "If you have a really long conversation or a lot of messages, you might exceed the max token limit of the context window allowed for the LLM when using `ConversationBufferMemory`\n",
        "\n",
        "`ConversationSummaryMemory` creates a summary of the conversation history over time. This can be useful for condensing information from the conversation messages over time.\n",
        "\n",
        "This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
      ],
      "metadata": {
        "id": "7TMX6YbDMrlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationSummaryMemory(return_messages=True, llm=chatgpt)\n",
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages as a summary to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "WTVkUbliMxlN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Explain AI in 2 bullet points'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5FioyeUNECZ",
        "outputId": "58c6a2c3-f221-43a2-a879-2264823cdfca"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\n",
            "2. AI technologies include machine learning, natural language processing, computer vision, and robotics, and are used in various applications like virtual assistants, autonomous vehicles, and medical diagnosis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Now do the same for Deep Learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvNdcV40NFE7",
        "outputId": "aef1f58b-f15c-4e8a-de5d-a5ea7af339bc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Deep learning is a subset of artificial intelligence that involves training artificial neural networks to learn and make decisions on their own.\n",
            "- It is used in various applications such as image and speech recognition, natural language processing, and autonomous vehicles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847K86fINF7H",
        "outputId": "d997ce8a-59c7-4fe3-9a9d-81d77beee8db"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='The human asks the AI to explain AI in 2 bullet points. The AI explains that AI refers to the simulation of human intelligence processes by machines and lists various AI technologies and applications. The human then asks the AI to do the same for Deep Learning. The AI explains that Deep Learning is a subset of artificial intelligence that involves training artificial neural networks to learn and make decisions on their own, and lists various applications of Deep Learning.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What have we discussed so far?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFUdQ_iUNG1M",
        "outputId": "d0df9ea5-bbfa-48ec-bb30-18637e941e61"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have discussed AI and Deep Learning, including their definitions and applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RiLwib1wNH-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}