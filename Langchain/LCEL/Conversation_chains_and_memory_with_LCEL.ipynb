{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Q9Y23CCXIv",
        "outputId": "20ae6a8d-2bb2-4f25-958a-134e118a158e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/973.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/973.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.2.0 --quiet\n",
        "!pip install langchain-openai==0.1.7 --quiet\n",
        "!pip install langchain-community==0.2.0 --quiet\n",
        "!pip install langchain-chroma==0.1.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_API_KEY')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "HTBjsHo1Cn8w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)"
      ],
      "metadata": {
        "id": "d0eM47tCCyUn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with LangChain Chains\n",
        "\n",
        "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. Also running on multiple data points can be done easily with chains.\n",
        "\n",
        "Chain's are the legacy interface for \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains.\n",
        "\n",
        "Here we will be using LCEL chains exclusively"
      ],
      "metadata": {
        "id": "sC7d5hIyDCpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Problem with Simple LLM Chains\n",
        "\n",
        "Simple LLM Chains cannot keep a track of past conversation history"
      ],
      "metadata": {
        "id": "i0aupmqQDSa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "prompt_txt = \"\"\"{query}\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_txt)\n",
        "\n",
        "llm_chain = (prompt\n",
        "             |\n",
        "             chatgpt\n",
        "             )"
      ],
      "metadata": {
        "id": "xZGZ-1DdDVkF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_chain.invoke({'query': 'What are the first four colors of a rainbow?'})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aly49GiTDtUX",
        "outputId": "6b2aa387-128a-4fbd-c883-7f297108eb6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first four colors of a rainbow are red, orange, yellow, and green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_chain.invoke({'query': 'and the other 3?'})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYEyzp8vDxDa",
        "outputId": "2a7e63b5-132e-46ab-aad1-9a6714e0025b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, I'm not sure what you are referring to. Can you please provide more context or clarify your question?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with LCEL\n",
        "\n",
        "LangChain Expression Language (LCEL) connects prompts, models, parsers and retrieval components using a `|` pipe operator.\n",
        "\n",
        "A conversation chain basically consists of user prompts, historical conversation memory and the LLM. The LLM uses the history memory to give more contextual answers to every new prompt or user query.\n",
        "\n",
        "Memory is very important for having a true conversation with LLMs. LangChain allows us to manage conversation memory using various constructs. The main ones we will cover include:\n",
        "\n",
        "- ConversationBufferMemory\n",
        "- ConversationBufferWindowMemory\n",
        "- ConversationSummaryMemory\n",
        "- VectorStoreRetrieverMemory\n",
        "- ChatMessageHistory\n",
        "- SQLChatMessageHistory"
      ],
      "metadata": {
        "id": "VvpCox-tD07X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with ConversationBufferMemory\n",
        "\n",
        "This is the simplest version of in-memory storage of historical conversation messages. It is basically a buffer for storing conversation memory.\n",
        "\n",
        "Remember if you have a really long conversation, you might exceed the max token limit of the context window allowed for the LLM."
      ],
      "metadata": {
        "id": "dSDiIfTEEcEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ],
      "metadata": {
        "id": "MirMMXsHEf5N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get historical conversation messages from the memory\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60--JuLcGyVx",
        "outputId": "c8e4caa6-8383-467e-da91-02e4c2e2ea5e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets create a function now which returns the list of messages from memory\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "get_memory_messages('What are the first four colors of a rainbow?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEz5JB4YG0gy",
        "outputId": "ebeafd27-803a-40b8-cd4f-7ef51d1328e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing out the function with a runnable lambda which will go into our chain\n",
        "# this returns the history but we also need to send our current query to the prompt\n",
        "RunnableLambda(get_memory_messages).invoke({'query': 'What are the first four colors of a rainbow?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "870gOaOpG-QE",
        "outputId": "d77d06ad-f7c4-4990-c6d9-e45412e5af39"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we use a runnable passthrough to pass our current query untouched\n",
        "# along with the history messages to the next step in the chain\n",
        "RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ).invoke({'query': 'What are the first four colors of a rainbow?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8LGrAefHAK-",
        "outputId": "9ef02145-a8e9-4b0e-9278-a32a2010efda"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What are the first four colors of a rainbow?', 'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "X54ssoU-HDu1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What are the first four colors of a rainbow?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-yJg4xPHHN4",
        "outputId": "e73d5ef9-f21e-438e-cfb8-0618efc9169a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 30, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-93699e16-adaf-4791-a73b-847abeeb5809-0')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peiuv6twHIoP",
        "outputId": "7d97cb97-f2fc-4fa6-bbad-54ba5e490025"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first four colors of a rainbow are red, orange, yellow, and green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txIO7e6MHOH9",
        "outputId": "b9c5382c-29e9-4b5c-9ef0-8f740e367f8a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(query, {\"output\": response.content})"
      ],
      "metadata": {
        "id": "a9-bnU3gHPmv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ct6kldyHRAz",
        "outputId": "725edd27-6b50-42aa-b169-0c1f81054bee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What are the first four colors of a rainbow?'),\n",
              "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'and the other 3?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvlagz3MHSYX",
        "outputId": "fae3c2ab-1fe6-4a8d-935e-94123d2ce9bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The other three colors of a rainbow are blue, indigo, and violet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qeds6pNgHU3v",
        "outputId": "2d454231-1abd-4e35-cfa8-ede097a92655"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What are the first four colors of a rainbow?'),\n",
              "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.'),\n",
              "  HumanMessage(content='and the other 3?'),\n",
              "  AIMessage(content='The other three colors of a rainbow are blue, indigo, and violet.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Explain AI in 2 bullet points'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v25vUOtfHWyS",
        "outputId": "f928d2cd-1e2e-4012-e842-4a72741501fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\n",
            "- AI technologies include machine learning, natural language processing, computer vision, and robotics, among others.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Now do the same for Deep Learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcX_FwhIHYkh",
        "outputId": "4e1de934-7799-4940-ac64-718323f8346e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Deep learning is a subset of artificial intelligence that uses neural networks with multiple layers to learn and make decisions from large amounts of data.\n",
            "- Deep learning is particularly effective for tasks such as image and speech recognition, natural language processing, and autonomous driving.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQv3CgwPHaYO",
        "outputId": "db2f0ce2-e72c-4efd-b054-bee25cf37345"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What are the first four colors of a rainbow?'),\n",
              "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.'),\n",
              "  HumanMessage(content='and the other 3?'),\n",
              "  AIMessage(content='The other three colors of a rainbow are blue, indigo, and violet.'),\n",
              "  HumanMessage(content='Explain AI in 2 bullet points'),\n",
              "  AIMessage(content='- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\\n- AI technologies include machine learning, natural language processing, computer vision, and robotics, among others.'),\n",
              "  HumanMessage(content='Now do the same for Deep Learning'),\n",
              "  AIMessage(content='- Deep learning is a subset of artificial intelligence that uses neural networks with multiple layers to learn and make decisions from large amounts of data.\\n- Deep learning is particularly effective for tasks such as image and speech recognition, natural language processing, and autonomous driving.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What have we discussed so far?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi16XW8YHcY6",
        "outputId": "f2908186-d66e-4373-83dc-b20c0c065521"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have discussed the colors of a rainbow, artificial intelligence (AI), and deep learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with ConversationBufferWindowMemory\n",
        "\n",
        "If you have a really long conversation, you might exceed the max token limit of the context window allowed for the LLM when using `ConversationBufferMemory` so `ConversationBufferWindowMemory` helps in just storing the last K conversations (one conversation piece is one user message and the corresponding AI message from the LLM) and thus helps you manage token limits and costs"
      ],
      "metadata": {
        "id": "ieOMs1TSHeOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# stores last 2 sets of human-AI conversations\n",
        "memory = ConversationBufferWindowMemory(return_messages=True, k=2)\n",
        "\n",
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "zJWzWOqSMflz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What are the first four colors of a rainbow?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edukYgIsMkZA",
        "outputId": "82b6fab4-e41c-4ac6-94db-56191076d72d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first four colors of a rainbow are red, orange, yellow, and green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'and the other 3?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTl6jlICMld2",
        "outputId": "e82decd2-ca8b-4360-9584-74d47e862dab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The other three colors of a rainbow are blue, indigo, and violet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Explain AI in 2 bullet points'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRPHgxy8Mm6s",
        "outputId": "8cfb9801-d332-4e0f-e099-8e5ea6db72f1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\n",
            "- AI technologies include machine learning, natural language processing, computer vision, and robotics, among others.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Now do the same for Deep Learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3khlHPEMoMJ",
        "outputId": "81c5375d-9207-4425-c3fa-510069f421ad"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems.\n",
            "- It is particularly effective for tasks such as image and speech recognition, natural language processing, and autonomous driving.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLlZCQ7YMo8_",
        "outputId": "6ea88db3-6268-4e4a-c762-18b7b660c937"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Explain AI in 2 bullet points'),\n",
              "  AIMessage(content='- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\\n- AI technologies include machine learning, natural language processing, computer vision, and robotics, among others.'),\n",
              "  HumanMessage(content='Now do the same for Deep Learning'),\n",
              "  AIMessage(content='- Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems.\\n- It is particularly effective for tasks such as image and speech recognition, natural language processing, and autonomous driving.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What have we discussed so far?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb_UhbK3Mp1c",
        "outputId": "9e1a02ea-de99-404a-a558-4333162437db"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have discussed artificial intelligence (AI) and deep learning, including their definitions and key characteristics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with ConversationSummaryMemory\n",
        "\n",
        "If you have a really long conversation or a lot of messages, you might exceed the max token limit of the context window allowed for the LLM when using `ConversationBufferMemory`\n",
        "\n",
        "`ConversationSummaryMemory` creates a summary of the conversation history over time. This can be useful for condensing information from the conversation messages over time.\n",
        "\n",
        "This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
      ],
      "metadata": {
        "id": "7TMX6YbDMrlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationSummaryMemory(return_messages=True, llm=chatgpt)\n",
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages as a summary to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "WTVkUbliMxlN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Explain AI in 2 bullet points'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5FioyeUNECZ",
        "outputId": "fe327ce3-de20-4350-ca63-aea8bc276594"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\n",
            "2. AI technologies include machine learning, natural language processing, computer vision, and robotics, and are used in various applications like virtual assistants, autonomous vehicles, and medical diagnosis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Now do the same for Deep Learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvNdcV40NFE7",
        "outputId": "aeb55b78-7bae-47a6-86fe-fa7b5cfe1006"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Deep learning is a subset of artificial intelligence that involves training artificial neural networks to learn and make decisions on their own.\n",
            "- It is used in various applications such as image and speech recognition, natural language processing, and autonomous vehicles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847K86fINF7H",
        "outputId": "568876df-bfdf-4dba-c769-ca4657f5b980"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='The human asks the AI to explain AI in 2 bullet points. The AI explains that AI refers to the simulation of human intelligence processes by machines and lists various AI technologies and applications. The human then asks the AI to do the same for Deep Learning. The AI explains that Deep Learning is a subset of artificial intelligence that involves training artificial neural networks to learn and make decisions on their own, and it is used in various applications such as image and speech recognition, natural language processing, and autonomous vehicles.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What have we discussed so far?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFUdQ_iUNG1M",
        "outputId": "516b4022-60f8-4e42-9da5-6914aedc7fd2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have discussed artificial intelligence (AI) and deep learning, including their definitions and applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with VectorStoreRetrieverMemory\n",
        "\n",
        "`VectorStoreRetrieverMemory` stores historical conversation messages in a vector store and queries the top-K most \"relevant\" history messages every time it is called.\n",
        "\n",
        "This differs from most of the other Memory classes in that it doesn't explicitly track the order of interactions but retrieves history based on embedding similarity to the current question or prompt.\n",
        "\n",
        "In this case, the \"docs\" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation."
      ],
      "metadata": {
        "id": "RiLwib1wNH-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "hfhq5vLXADsS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a Vector Database to store conversation history\n",
        "\n",
        "Here we use the Chroma vector DB and initialize an empty database collection to store conversation messages"
      ],
      "metadata": {
        "id": "puA6XAmuA7Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# create empty vector DB\n",
        "chroma_db = Chroma(collection_name='history_db',\n",
        "                   embedding_function=openai_embed_model)"
      ],
      "metadata": {
        "id": "j6_MEPgXA9zn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# load 2 most similar conversation messages from vector db history for each new message \\ prompt\n",
        "# this uses cosine embedding similarity to load the top 2 similar messgages to the input prompt \\ query\n",
        "retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                   search_kwargs={\"k\": 2})\n",
        "memory = VectorStoreRetrieverMemory(retriever=retriever, return_messages=True)\n",
        "\n",
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return [memory.load_memory_variables(query)['history']]\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "q8sYRQXdA_lw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Tell me about AI'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtRGuCEnBK-t",
        "outputId": "4a54a774-344b-42c8-cec4-a7ab02f93d49"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems. AI technologies enable machines to learn from experience, adjust to new inputs, and perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI is used in various industries, including healthcare, finance, transportation, and entertainment, to improve efficiency, accuracy, and decision-making processes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What about deep learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0qfw9ZeBMa6",
        "outputId": "9ada2457-8058-4e55-9972-c7f8efb982ba"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 2 is greater than number of elements in index 1, updating n_results = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep learning is a subset of artificial intelligence that involves training artificial neural networks with multiple layers to learn and make decisions on its own. It is inspired by the structure and function of the human brain and is particularly effective for tasks such as image and speech recognition, natural language processing, and autonomous driving. Deep learning has shown significant advancements in recent years and is a key technology driving many AI applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Tell me about the fastest animal in the world in 2 lines'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umWoyP58BOZV",
        "outputId": "e7ed43a2-0f0d-45e6-9dc6-d3fd4b10c51b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The fastest animal in the world is the peregrine falcon, which can reach speeds of over 240 miles per hour when diving to catch prey. Its incredible speed and agility make it one of the most efficient hunters in the animal kingdom.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What about the cheetah?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHwPeKliBQiH",
        "outputId": "878590ab-9c1e-411a-9e81-11807d665c51"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cheetah is the fastest land animal, capable of reaching speeds up to 60-70 miles per hour in short bursts. Its incredible speed and agility make it a formidable predator, allowing it to chase down prey with remarkable efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.load_memory_variables({'query': 'What about machine learning?'})['history'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEJpUPI-BRjx",
        "outputId": "fdf76ac1-9b79-4c1d-b1fd-2aa5b457d500"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query: What about deep learning\n",
            "output: Deep learning is a subset of artificial intelligence that involves training artificial neural networks with multiple layers to learn and make decisions on its own. It is inspired by the structure and function of the human brain and is particularly effective for tasks such as image and speech recognition, natural language processing, and autonomous driving. Deep learning has shown significant advancements in recent years and is a key technology driving many AI applications.\n",
            "query: Tell me about AI\n",
            "output: AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems. AI technologies enable machines to learn from experience, adjust to new inputs, and perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI is used in various industries, including healthcare, finance, transportation, and entertainment, to improve efficiency, accuracy, and decision-making processes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What about machine learning?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQPtCyaNBTxD",
        "outputId": "05d522c3-5e7d-4acb-f6c5-b898b5e876e6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is a subset of artificial intelligence that focuses on developing algorithms and models that enable computers to learn from and make predictions or decisions based on data. It involves training machines to recognize patterns in data and make informed decisions without being explicitly programmed to do so. Machine learning algorithms are used in various applications, such as recommendation systems, predictive analytics, fraud detection, and autonomous vehicles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-user Conversation Chains with ChatMessageHistory\n",
        "\n",
        "The concept of `ChatHistory` refers to a class in LangChain which can be used to wrap an arbitrary chain. This `ChatHistory` will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database. Future interactions will then load those messages and pass them into the chain as part of the input.\n",
        "\n",
        "The beauty of `ChatMessageHistory` is that we can store separate conversation histories per user or session which is often the need for real-world chatbots which will be accessed by many users at the same time.\n",
        "\n",
        "We use a `get_session_history` function which is expected to take in a `session_id` and return a Message History object. Everything is stored in memory. This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain"
      ],
      "metadata": {
        "id": "rE7VdaucBYkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# used to retrieve conversation history from memory\n",
        "# based on a specific user or session ID\n",
        "history_store = {}\n",
        "def get_session_history(session_id):\n",
        "    if session_id not in history_store:\n",
        "        history_store[session_id] = ChatMessageHistory()\n",
        "    return history_store[session_id]\n",
        "\n",
        "# prompt to load in history and current input from the user\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Act as a helpful AI Assistant\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{human_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# create a basic LLM Chain\n",
        "llm_chain = (prompt_template\n",
        "                |\n",
        "             chatgpt)\n",
        "\n",
        "# create a conversation chain which can load memory based on specific user or session id\n",
        "conv_chain = RunnableWithMessageHistory(\n",
        "    llm_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"human_input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# create a utility function to take in current user input prompt and their session ID\n",
        "# streams result live back to the user from the LLM\n",
        "def chat_with_llm(prompt: str, session_id: str):\n",
        "    for chunk in conv_chain.stream({\"human_input\": prompt},\n",
        "                                   {'configurable': { 'session_id': session_id}}):\n",
        "        print(chunk.content, end=\"\")"
      ],
      "metadata": {
        "id": "VTBOvVSMBcID"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'bob123'\n",
        "prompt = \"Hi I am Bob, can you explain AI in 3 bullet points?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3d-32FtCFZP",
        "outputId": "f72e499f-8bbb-4cb2-b283-2a3f8634acc5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\n",
            "- AI technologies include machine learning, neural networks, natural language processing, and computer vision, among others.\n",
            "- AI is used in various applications, such as virtual assistants, autonomous vehicles, healthcare diagnostics, and fraud detection, to improve efficiency and decision-making."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Now do the same for deep learning\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcjDHND4CIm0",
        "outputId": "df9d87fb-79cb-443b-cc2a-e4f8b4abfa86"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems.\n",
            "- Deep learning algorithms are designed to automatically learn and extract features from data, without the need for explicit programming.\n",
            "- Deep learning has been successful in tasks such as image and speech recognition, natural language processing, and autonomous driving, due to its ability to handle large amounts of data and learn intricate patterns."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Discuss briefly what have we discussed so far is bullet points?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUjFZW9OCKQS",
        "outputId": "93e332b5-68c4-4e2c-a23a-a154c32c962e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- AI refers to the simulation of human intelligence processes by machines, utilizing technologies like machine learning, neural networks, and natural language processing.\n",
            "- Deep learning is a subset of AI that uses artificial neural networks to automatically learn and extract features from data, enabling complex problem-solving without explicit programming.\n",
            "- Both AI and deep learning have diverse applications in various fields, such as virtual assistants, healthcare diagnostics, image recognition, and autonomous driving, to enhance efficiency and decision-making processes."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'james007'\n",
        "prompt = \"Hi can you explain what is an LLM in 2 bullet points?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9pp16ZOCLv6",
        "outputId": "1a93bb82-c654-4e06-c4f2-182007e7e380"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- LLM stands for Master of Laws, which is a postgraduate degree typically pursued by individuals who already hold a law degree and wish to specialize in a specific area of law.\n",
            "- The LLM program allows students to deepen their knowledge and understanding of legal principles, gain expertise in a particular legal field, and enhance their career prospects in the legal profession."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Actually I meant in the context of AI?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoXKuZmdCNk5",
        "outputId": "f65b0c52-b5d5-45bd-a29a-c2d4f46c893d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- In the context of AI, LLM can refer to Large Language Models, which are advanced AI models designed to understand and generate human language at a sophisticated level.\n",
            "- LLMs, such as GPT-3 (Generative Pre-trained Transformer 3), have been developed by companies like OpenAI and are used for various applications, including natural language processing, text generation, and conversational AI."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Summarize briefly what we have discussed so far?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEN5S5_MCPBQ",
        "outputId": "fd24d049-a0bd-4d53-eac3-efab5ed1ba1a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have discussed two different meanings of LLM: Master of Laws (LLM) as a postgraduate degree for legal specialization, and Large Language Models (LLMs) in the context of AI as advanced models for understanding and generating human language."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'bob123'\n",
        "prompt = \"Discuss briefly what have we discussed so far is bullet points?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U3VGp2ICRlC",
        "outputId": "33363d7c-bab5-41bc-a5ae-5d6012629c27"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- AI refers to the simulation of human intelligence processes by machines, utilizing technologies like machine learning, neural networks, and natural language processing.\n",
            "- Deep learning is a subset of AI that uses artificial neural networks to automatically learn and extract features from data, enabling complex problem-solving without explicit programming.\n",
            "- Both AI and deep learning have diverse applications in various fields, such as virtual assistants, healthcare diagnostics, image recognition, and autonomous driving, to enhance efficiency and decision-making processes."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-user Window-based Conversation Chains with persistence - SQLChatMessageHistory\n",
        "\n",
        "The beauty of `SQLChatMessageHistory` is that we can store separate conversation histories per user or session which is often the need for real-world chatbots which will be accessed by many users at the same time. Instead of in-memory we can store it in a SQL database which can be used to store a lot of conversations.\n",
        "\n",
        "We use a `get_session_history` function which is expected to take in a `session_id` and return a Message History object. Everything is stored in a SQL database. This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain\n",
        "\n",
        "We also use a `memory_buffer_window` function to only use the top-K last historical conversations before sending it to the LLM, basically our own implementation of `ConversationBufferWindowMemory`"
      ],
      "metadata": {
        "id": "U-eMEb8ECU2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removes the memory database file - usually not needed\n",
        "# you can run this only when you want to remove all conversation histories\n",
        "!rm memory.db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6JLPUe7CbJz",
        "outputId": "7209f588-5eb6-4463-bb24-e4b945eb1411"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'memory.db': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# used to retrieve conversation history from database\n",
        "# based on a specific user or session ID\n",
        "def get_session_history_db(session_id):\n",
        "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n",
        "\n",
        "# prompt to load in history and current input from the user\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Act as a helpful AI Assistant\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{human_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# create a memory buffer window function to return the last K conversations\n",
        "def memory_buffer_window(messages, k=2):\n",
        "    return messages[-(k+1):]\n",
        "\n",
        "# create a basic LLM Chain which only sends the last K conversations per user\n",
        "llm_chain = (\n",
        "    RunnablePassthrough.assign(history=lambda x: memory_buffer_window(x[\"history\"]))\n",
        "      |\n",
        "    prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        ")"
      ],
      "metadata": {
        "id": "1e90GTKrCoBC"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a conversation chain which can load memory based on specific user or session id\n",
        "conv_chain = RunnableWithMessageHistory(\n",
        "    llm_chain,\n",
        "    get_session_history_db,\n",
        "    input_messages_key=\"human_input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# create a utility function to take in current user input prompt and their session ID\n",
        "# streams result live back to the user from the LLM\n",
        "def chat_with_llm(prompt: str, session_id: str):\n",
        "    for chunk in conv_chain.stream({\"human_input\": prompt},\n",
        "                                   {'configurable': { 'session_id': session_id}}):\n",
        "        print(chunk.content, end=\"\")"
      ],
      "metadata": {
        "id": "Yce2yKuqCr7s"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'jim001'\n",
        "prompt = \"Hi can you tell me which is the fastest animal?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxX2mMwMC5mw",
        "outputId": "e761cb4f-8055-4b5a-a147-e0ef6484c080"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The fastest animal on land is the cheetah, which can reach speeds of up to 60-70 miles per hour (96-112 km/h) in short bursts covering distances up to 500 meters. In the air, the peregrine falcon holds the title for the fastest animal, reaching speeds of over 240 miles per hour (386 km/h) when diving to catch prey."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what about the slowest animal?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMdy-PqZC7SI",
        "outputId": "3c46f943-acb2-4626-d445-0fddb9fa9bed"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest animal is the garden snail, which moves at a leisurely pace of about 0.03 miles per hour (0.048 km/h). Garden snails are known for their slow and deliberate movements, using their muscular foot to glide along a trail of mucus they secrete."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what about the largest animal?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRi9tMG7C8c1",
        "outputId": "983926b6-60ba-4684-c2dc-bee87b3043b1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The largest animal on Earth is the blue whale. Blue whales can grow up to 100 feet (30 meters) in length and weigh as much as 200 tons. These magnificent creatures are known for their massive size and are the largest animals to have ever lived on our planet."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what topics have we discussed, show briefly as bullet points\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ogiUlkAC-5Z",
        "outputId": "d7184ccd-8574-46f0-ab53-3526ad67bae9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- The slowest animal\n",
            "- The largest animal"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'john005'\n",
        "prompt = \"Explain AI in 3 bullets to a child\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrCu_untDAdF",
        "outputId": "20146c2e-882e-43ea-af50-5312c893d2e3"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- AI is like a smart robot that can learn and think on its own.\n",
            "- It can help us do things faster and better, like playing games or finding information.\n",
            "- AI is like having a super smart friend who can help us with different tasks."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Now do the same for Generative AI\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7lF4sGcDB-e",
        "outputId": "ba114a70-433d-418a-c9e1-92f6168eb137"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Generative AI is a special type of AI that can create new things, like art or music.\n",
            "- It uses algorithms to come up with new ideas and designs.\n",
            "- Generative AI is like a magical artist that can make beautiful things out of nothing."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Now do the same for machine learning\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dqvPnzlDD-6",
        "outputId": "32b93b31-1ee3-4686-8bb0-ccb27c907212"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Machine learning is a type of AI that can learn from data and improve over time.\n",
            "- It uses algorithms to analyze patterns in data and make predictions or decisions.\n",
            "- Machine learning is like having a super smart assistant that can help us make sense of complex information and make better decisions."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what topics have we discussed, show briefly as bullet points\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL6j8mZ_DE9x",
        "outputId": "a61f8d8a-8b38-4049-ac94-12dda6a494ff"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Generative AI: AI that can create new things like art or music using algorithms.\n",
            "- Machine learning: AI that can learn from data, analyze patterns, and make predictions or decisions to improve over time."
          ]
        }
      ]
    }
  ]
}