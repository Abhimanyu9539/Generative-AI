{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## QA RAG"
      ],
      "metadata": {
        "id": "hP01w5uwBcYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.12 --quiet\n",
        "!pip install langchain-openai==0.0.8 --quiet\n",
        "!pip install langchain-community==0.0.29 --quiet\n",
        "!pip install streamlit==1.32.2 --quiet\n",
        "!pip install PyMuPDF==1.24.0 --quiet\n",
        "!pip install chromadb==0.4.24 --quiet\n",
        "!pip install pyngrok==7.1.5 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikcr94bECLKR",
        "outputId": "9fe0c35d-d0a4-4c1d-e254-30b6b16d0a79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "streamlit 1.32.2 requires protobuf<5,>=3.20, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_API_KEY')\n",
        "os.environ['NGROK_AUTH_TOKEN'] = userdata.get('NGROK_AUTH_TOKEN')"
      ],
      "metadata": {
        "id": "ZD5LVm_TC69c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_core.callbacks.base import BaseCallbackHandler\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "from operator import itemgetter\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Customize initial app landing page\n",
        "st.set_page_config(page_title=\"File QA Chatbot\", page_icon=\"ğŸ¤–\")\n",
        "st.title(\"Welcome to File QA RAG Chatbot ğŸ¤–\")\n",
        "\n",
        "@st.cache_resource(ttl=\"1h\")\n",
        "# Takes uploaded PDFs, creates document chunks, computes embeddings\n",
        "# Stores document chunks and embeddings in a Vector DB\n",
        "# Returns a retriever which can look up the Vector DB\n",
        "# to return documents based on user input\n",
        "# Stores this in the cache\n",
        "def configure_retriever(uploaded_files):\n",
        "  # Read documents\n",
        "  docs = []\n",
        "  temp_dir = tempfile.TemporaryDirectory()\n",
        "  for file in uploaded_files:\n",
        "    temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "    with open(temp_filepath, \"wb\") as f:\n",
        "      f.write(file.getvalue())\n",
        "    loader = PyMuPDFLoader(temp_filepath)\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "  # Split into documents chunks\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
        "                                                 chunk_overlap=200)\n",
        "  doc_chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "  # Create document embeddings and store in Vector DB\n",
        "  embeddings_model = OpenAIEmbeddings()\n",
        "  vectordb = Chroma.from_documents(doc_chunks, embeddings_model)\n",
        "\n",
        "  # Define retriever object\n",
        "  retriever = vectordb.as_retriever()\n",
        "  return retriever\n",
        "\n",
        "# Manages live updates to a Streamlit app's display by appending new text tokens\n",
        "# to an existing text stream and rendering the updated text in Markdown\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "  def __init__(self, container, initial_text=\"\"):\n",
        "    self.container = container\n",
        "    self.text = initial_text\n",
        "\n",
        "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "    self.text += token\n",
        "    self.container.markdown(self.text)\n",
        "\n",
        "# Creates UI element to accept PDF uploads\n",
        "uploaded_files = st.sidebar.file_uploader(\n",
        "  label=\"Upload PDF files\", type=[\"pdf\"],\n",
        "  accept_multiple_files=True\n",
        ")\n",
        "if not uploaded_files:\n",
        "  st.info(\"Please upload PDF documents to continue.\")\n",
        "  st.stop()\n",
        "\n",
        "# Create retriever object based on uploaded PDFs\n",
        "retriever = configure_retriever(uploaded_files)\n",
        "\n",
        "# Load a connection to ChatGPT LLM\n",
        "chatgpt = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.1,\n",
        "                     streaming=True)\n",
        "\n",
        "# Create a prompt template for QA RAG System\n",
        "qa_template = \"\"\"\n",
        "              Use only the following pieces of context to answer the question at the end.\n",
        "              If you don't know the answer, just say that you don't know,\n",
        "              don't try to make up an answer. Keep the answer as concise as possible.\n",
        "\n",
        "              {context}\n",
        "\n",
        "              Question: {question}\n",
        "              \"\"\"\n",
        "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
        "\n",
        "# This function formats retrieved documents before sending to LLM\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "# Create a QA RAG System Chain\n",
        "qa_rag_chain = (\n",
        "  {\n",
        "    \"context\": itemgetter(\"question\") # based on the user question get context docs\n",
        "      |\n",
        "    retriever\n",
        "      |\n",
        "    format_docs,\n",
        "    \"question\": itemgetter(\"question\") # user question\n",
        "  }\n",
        "    |\n",
        "  qa_prompt # prompt with above user question and context\n",
        "    |\n",
        "  chatgpt # above prompt is sent to the LLM for response\n",
        ")\n",
        "\n",
        "# Store conversation history in Streamlit session state\n",
        "streamlit_msg_history = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
        "\n",
        "# Shows the first message when app starts\n",
        "if len(streamlit_msg_history.messages) == 0:\n",
        "  streamlit_msg_history.add_ai_message(\"Please ask your question?\")\n",
        "\n",
        "# Render current messages from StreamlitChatMessageHistory\n",
        "for msg in streamlit_msg_history.messages:\n",
        "  st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "# Callback handler which does some post-processing on the LLM response\n",
        "# Used to post the top 3 document sources used by the LLM in RAG response\n",
        "class PostMessageHandler(BaseCallbackHandler):\n",
        "  def __init__(self, msg: st.write):\n",
        "    BaseCallbackHandler.__init__(self)\n",
        "    self.msg = msg\n",
        "    self.sources = []\n",
        "\n",
        "  def on_retriever_end(self, documents, *, run_id, parent_run_id, **kwargs):\n",
        "    source_ids = []\n",
        "    for d in documents: # retrieved documents from retriever based on user query\n",
        "      metadata = {\n",
        "        \"source\": d.metadata[\"source\"],\n",
        "        \"page\": d.metadata[\"page\"],\n",
        "        \"content\": d.page_content[:200]\n",
        "      }\n",
        "      idx = (metadata[\"source\"], metadata[\"page\"])\n",
        "      if idx not in source_ids: # store unique source documents\n",
        "        source_ids.append(idx)\n",
        "        self.sources.append(metadata)\n",
        "\n",
        "  def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
        "    if len(self.sources):\n",
        "      st.markdown(\"__Sources:__ \"+\"\\n\")\n",
        "      st.dataframe(data=pd.DataFrame(self.sources[:3]),\n",
        "                    width=1000) # Top 3 sources\n",
        "\n",
        "\n",
        "# If user inputs a new prompt, display it and show the response\n",
        "if user_prompt := st.chat_input():\n",
        "  st.chat_message(\"human\").write(user_prompt)\n",
        "  # This is where response from the LLM is shown\n",
        "  with st.chat_message(\"ai\"):\n",
        "    # Initializing an empty data stream\n",
        "    stream_handler = StreamHandler(st.empty())\n",
        "    # UI element to write RAG sources after LLM response\n",
        "    sources_container = st.write(\"\")\n",
        "    pm_handler = PostMessageHandler(sources_container)\n",
        "    config = {\"callbacks\": [stream_handler, pm_handler]}\n",
        "    # Get LLM response\n",
        "    response = qa_rag_chain.invoke({\"question\": user_prompt},\n",
        "                                    config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bExMSikQDQUA",
        "outputId": "80bd0edc-2e36-44a3-e3be-cc41a081bece"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "DiY5kfsdDa7n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import yaml\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken\n",
        "# Get your authtoken from `ngrok_credentials.yml` file\n",
        "# with open('ngrok_credentials.yml', 'r') as file:\n",
        "#     NGROK_AUTH_TOKEN = yaml.safe_load(file)\n",
        "ngrok.set_auth_token(os.environ['NGROK_AUTH_TOKEN'] )\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYEeu_G9DcyI",
        "outputId": "1eadbad5-c238-4f71-d73c-f735b2d14c03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://126c-34-148-254-146.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}